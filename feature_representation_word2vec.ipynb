{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b2681bb-a91a-42da-ba8e-9a999c7f8810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import ast\n",
    "import re\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "#data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "#NLP & ML libraries\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from textblob import TextBlob\n",
    "from nltk import FreqDist\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, InterclusterDistance\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bbe50fc-de36-41b1-993b-1044cc1b8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed so that code output is deterministic\n",
    "random.seed(30)  # Set the seed for Python's random module\n",
    "np.random.seed(30)  # Set the seed for NumPy's random module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7de6faf-08e5-4b9a-9237-4b6f1a2e6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cleaned data\n",
    "\n",
    "def list_converter(text):\n",
    "    #to revert list->str conversion from pd.read_csv\n",
    "    return ast.literal_eval(text)\n",
    "\n",
    "\n",
    "data = pd.read_csv('Data/training_corpus.csv', converters ={'tokens':list_converter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4226a4d3-22a8-4ebe-b077-575d16a727a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99186, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfo2hl</td>\n",
       "      <td>2021</td>\n",
       "      <td>*Cuntry roads, take me hoem*</td>\n",
       "      <td>cuntry roads hoem</td>\n",
       "      <td>3</td>\n",
       "      <td>[cuntry, road, hoem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfqkbv</td>\n",
       "      <td>2021</td>\n",
       "      <td>That’s been there for several years, sent a pi...</td>\n",
       "      <td>years sent pic cuntry friend long time ago</td>\n",
       "      <td>8</td>\n",
       "      <td>[year, send, pic, cuntry, friend, long, time, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfou07</td>\n",
       "      <td>2021</td>\n",
       "      <td>I am single and I have not traveled to any cun...</td>\n",
       "      <td>single traveled cuntry past year</td>\n",
       "      <td>5</td>\n",
       "      <td>[single, travel, cuntry, past, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfrgpe</td>\n",
       "      <td>2021</td>\n",
       "      <td>What happens when you shop at dragon mart...</td>\n",
       "      <td>happens shop dragon mart</td>\n",
       "      <td>4</td>\n",
       "      <td>[happen, shop, dragon, mart]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>gthiiwi</td>\n",
       "      <td>2021</td>\n",
       "      <td>That’s just absolutely hilarious, is this in t...</td>\n",
       "      <td>absolutely hilarious springs souk</td>\n",
       "      <td>4</td>\n",
       "      <td>[absolutely, hilarious, spring, souk]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text_type       ID  year                                          long_text  \\\n",
       "0   comment  gtfo2hl  2021                       *Cuntry roads, take me hoem*   \n",
       "1   comment  gtfqkbv  2021  That’s been there for several years, sent a pi...   \n",
       "2   comment  gtfou07  2021  I am single and I have not traveled to any cun...   \n",
       "3   comment  gtfrgpe  2021       What happens when you shop at dragon mart...   \n",
       "4   comment  gthiiwi  2021  That’s just absolutely hilarious, is this in t...   \n",
       "\n",
       "                                   clean_text  word_count  \\\n",
       "0                           cuntry roads hoem           3   \n",
       "1  years sent pic cuntry friend long time ago           8   \n",
       "2            single traveled cuntry past year           5   \n",
       "3                    happens shop dragon mart           4   \n",
       "4           absolutely hilarious springs souk           4   \n",
       "\n",
       "                                              tokens  \n",
       "0                               [cuntry, road, hoem]  \n",
       "1  [year, send, pic, cuntry, friend, long, time, ...  \n",
       "2               [single, travel, cuntry, past, year]  \n",
       "3                       [happen, shop, dragon, mart]  \n",
       "4              [absolutely, hilarious, spring, souk]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns = ['index'])\n",
    "print (data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a90b0-730e-445f-8b85-31be72b739e4",
   "metadata": {},
   "source": [
    "## **Word2Vec Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "684d9375-5fc5-4c94-95b3-fc3fcbbed923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cuntry', 'road', 'hoem'],\n",
       " ['year', 'send', 'pic', 'cuntry', 'friend', 'long', 'time', 'ago'],\n",
       " ['single', 'travel', 'cuntry', 'past', 'year'],\n",
       " ['happen', 'shop', 'dragon', 'mart'],\n",
       " ['absolutely', 'hilarious', 'spring', 'souk']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert df['tokens'] to list of strings for word2vec model\n",
    "corpus = data['tokens'].tolist()#.apply(lambda token: ' '.join(token)).tolist()\n",
    "\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45ea010-a712-4035-8952-fc3fbef3a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time() #track start time of execution\n",
    "\n",
    "#code from codebasics and semicolon, both on youtube\n",
    "\n",
    "#instantiate word2vec model\n",
    "word2vec = Word2Vec (window = 5, min_count = 2, workers = 4)\n",
    "   \n",
    "#build a vocabulary\n",
    "word2vec.build_vocab(corpus, progress_per = 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d745b77-a89b-403c-b2b7-fc4c185005c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99186"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da988379-baa1-4cf0-b11c-622c29f68051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6609385, 7045680)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the word2vec model\n",
    "\n",
    "word2vec.train(\n",
    "                corpus,\n",
    "                total_examples = word2vec.corpus_count,\n",
    "                epochs = word2vec.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fa63580-bc00-499d-a96d-e308cb515b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nesto', 0.9561541080474854),\n",
       " ('cafeteria', 0.9483346343040466),\n",
       " ('waitrose', 0.9481028318405151),\n",
       " ('veggie', 0.9419034719467163),\n",
       " ('spinney', 0.9393037557601929),\n",
       " ('carrefour', 0.9377502202987671),\n",
       " ('hypermarket', 0.9365057349205017),\n",
       " ('fish', 0.9346320033073425),\n",
       " ('fruit', 0.9335165023803711),\n",
       " ('vegetable', 0.932704746723175)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar ('lulu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1ad9ba5-27af-4c07-87ce-bb21a6cc2ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('palace', 0.8755813837051392),\n",
       " ('eg', 0.867764949798584),\n",
       " ('madinah', 0.8672471046447754)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(positive =['king', 'woman'], negative=['man'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dcf2763-e97d-46c6-be2f-0036c183c593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.doesnt_match(['ejari', 'rera', 'dewa', 'drive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574248b-ba20-490a-b128-02740445be03",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Word2Vec Model** with custom stopwords removed from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a41365-01da-4fc2-91ec-3ddf56de1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#created the filtered corpus from another notebook and imported it here for use\n",
    "# Read the file\n",
    "with open('updated_corpus.txt', 'r') as f:\n",
    "    updated_data = f.readlines()\n",
    "\n",
    "# Remove newline characters\n",
    "updated_data = [line.strip() for line in updated_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "399a79f4-e118-49c1-b632-dcbe3f1be931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated_data is a <class 'list'>, and the first five elements are:\n",
      " ['road', '', 'single travel past', 'shop dragon mart', 'hilarious spring souk']\n"
     ]
    }
   ],
   "source": [
    "print (f'updated_data is a {type(updated_data)}, and the first five elements are:\\n',updated_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bc3f6c1-c4ee-46a1-b9ce-3a4a91ecb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list of strings to dataframe column with list of tokens on each row\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "updated_data = pd.DataFrame(updated_data, columns = ['tokens'])\n",
    "updated_data = updated_data['tokens'].apply(lambda x: list(tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6755b26-876d-42e5-b542-efc76618abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                       [road]\n",
       "1                           []\n",
       "2       [single, travel, past]\n",
       "3         [shop, dragon, mart]\n",
       "4    [hilarious, spring, souk]\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(updated_data))\n",
    "updated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fd6e35d-4d81-4e71-8998-c74b78511486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['road'],\n",
       " [],\n",
       " ['single', 'travel', 'past'],\n",
       " ['shop', 'dragon', 'mart'],\n",
       " ['hilarious', 'spring', 'souk']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert ['tokens'] column to list of strings for word2vec model\n",
    "updated_corpus = updated_data.tolist()#.apply(lambda token: ' '.join(token)).tolist()\n",
    "\n",
    "updated_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092febb8-13a9-421b-925f-e97b8966ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time() #track start time of execution\n",
    "\n",
    "#code from codebasics and semicolon, both on youtube\n",
    "\n",
    "#instantiate word2vec model\n",
    "word2vec2 = Word2Vec (window = 5, min_count = 2, workers = 4)\n",
    "   \n",
    "#build a vocabulary\n",
    "word2vec2.build_vocab(corpus, progress_per = 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda49e32-129d-4283-b9e9-05fb172e724f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99186"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of sentences in vocabulary\n",
    "word2vec2.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7146b649-752a-47fd-8a0a-f7e15709de0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4198495, 4198495)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the word2vec model\n",
    "\n",
    "word2vec2.train(\n",
    "                updated_corpus,\n",
    "                total_examples = word2vec2.corpus_count,\n",
    "                epochs = word2vec2.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e684cb44-9a8d-449d-9f9a-f6a2f41be75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('waitrose', 0.9908475875854492),\n",
       " ('spinney', 0.9897341728210449),\n",
       " ('hypermarket', 0.9820606112480164),\n",
       " ('mcdonalds', 0.980934739112854),\n",
       " ('carrefour', 0.9793255925178528),\n",
       " ('nesto', 0.9780416488647461),\n",
       " ('strawberry', 0.9738538265228271),\n",
       " ('coop', 0.9728703498840332),\n",
       " ('wholesale', 0.9718944430351257),\n",
       " ('vegetable', 0.969465434551239)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec2.wv.most_similar ('lulu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80568523-7529-484b-bca3-681424544c95",
   "metadata": {},
   "source": [
    "## **Word2Vec with bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7193727-a947-4bf0-8d34-2404047b1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first model on full corpus\n",
    "\n",
    "#code from gensim documentation\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "#train bigram detector\n",
    "bigram_transform = Phrases (corpus, min_count = 1) #default threshold is 10.0, fewer phrases will be created\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be3aceb2-7f4a-4d99-a8b6-90d07d67b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anymore', 'new', 'labor_law', 'common', 'blatant', 'good', 'company', 'care', 'sale', 'promotional', 'bdm', 'post', 'hire', 'look', 'filter', 'photo_cv', 'rife', 'real_estate', 'industry', 'say', 'uae', 'young', 'country', 'learn', 'motivation', 'law', 'change', 'align', 'progressive', 'practice', 'ahead', 'way', 'nature', 'place', 'live']\n"
     ]
    }
   ],
   "source": [
    "#check performance of bigram transform model\n",
    "new_sentence = corpus[330]\n",
    "print(bigram_transform[new_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e64e8cca-b521-4c74-b44e-d5e681a6b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply bi-gram transform on corpus for word2vec model\n",
    "\n",
    "bigram_corpus = [bigram_transform[sentence] for sentence in corpus]\n",
    "\n",
    "bigram_word2vec = Word2Vec (bigram_corpus,window = 5, min_count = 2, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f21cd84-4fc9-45a9-b6eb-0f464d6731ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tolerant', 0.9880362749099731),\n",
       " ('lgbtq', 0.9879887104034424),\n",
       " ('respectful', 0.9858818650245667),\n",
       " ('criticize', 0.982750415802002),\n",
       " ('proud', 0.9826979637145996),\n",
       " ('rape', 0.9824931025505066),\n",
       " ('feminism', 0.9823785424232483),\n",
       " ('defend', 0.9820605516433716),\n",
       " ('liberal', 0.981878936290741),\n",
       " ('attack', 0.9818695187568665)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_word2vec.wv.most_similar('lgbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dd6ef11-afae-4e9d-a41b-760599b2ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second model on filtered corpus --- additonal stopwords removed (informal conversation common words)\n",
    "\n",
    "#train bigram detector\n",
    "bigram_transform2 = Phrases (updated_corpus, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "468807c3-66c1-4e2e-821a-588c1a4a2069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anymore', 'new', 'labor_law', 'common', 'blatant', 'good', 'company', 'care', 'sale', 'promotional', 'bdm', 'post', 'hire', 'look', 'filter', 'photo_cv', 'rife', 'real_estate', 'industry', 'say', 'uae', 'young', 'country', 'learn', 'motivation', 'law', 'change', 'align', 'progressive', 'practice', 'ahead', 'way', 'nature', 'place', 'live']\n"
     ]
    }
   ],
   "source": [
    "#check performance of bigram transform model\n",
    "new_sentence = corpus[330]\n",
    "print(bigram_transform[new_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "430d0634-ec61-4051-967d-f9a8ffb2f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply bi-gram transform on corpus for word2vec model\n",
    "\n",
    "bigram_corpus2 = [bigram_transform2[sentence] for sentence in corpus]\n",
    "\n",
    "bigram_word2vec2 = Word2Vec (bigram_corpus2,window = 5, min_count = 2, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd7ff264-f4a1-4ca0-a591-a50d03257c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lgbtq', 0.9786578416824341),\n",
       " ('intolerant', 0.9772281646728516),\n",
       " ('freedom_speech', 0.9766908288002014),\n",
       " ('hypocritical', 0.9748409986495972),\n",
       " ('tolerate', 0.974479615688324),\n",
       " ('agenda', 0.9733495712280273),\n",
       " ('violence', 0.9730541706085205),\n",
       " ('brainwash', 0.9726577997207642),\n",
       " ('extremist', 0.9689884781837463),\n",
       " ('anti_muslim', 0.9688097834587097)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_word2vec2.wv.most_similar('lgbt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eee94d-be89-4159-b53c-1cd405356990",
   "metadata": {},
   "source": [
    "## **Word2Vec: Transfer Learning with wiki**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c101d7d-d00e-4090-8b35-f6469436d655",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/pretrained_models/glove-wiki_gigaword-100'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[0;32m----> 2\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/pretrained_models/glove-wiki_gigaword-100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/gensim/utils.py:486\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    482\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, fname)\n\u001b[1;32m    484\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[0;32m--> 486\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m obj\u001b[38;5;241m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[1;32m    488\u001b[0m obj\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname\u001b[38;5;241m=\u001b[39mfname)\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/gensim/utils.py:1460\u001b[0m, in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munpickle\u001b[39m(fname):\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \n\u001b[1;32m   1449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \n\u001b[1;32m   1459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1460\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1461\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _pickle\u001b[38;5;241m.\u001b[39mload(f, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/smart_open/smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/smart_open/smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    361\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/pretrained_models/glove-wiki_gigaword-100'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "pretrained_model = KeyedVectors.load(\"/pretrained_models/glove-wiki_gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "796d21e9-ed03-481c-bbe2-52ee6803357f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transgender', 0.8725160956382751),\n",
       " ('lesbian', 0.8088030815124512),\n",
       " ('bisexual', 0.7536906003952026),\n",
       " ('gay', 0.6802830696105957),\n",
       " ('glbt', 0.6522399187088013),\n",
       " ('lgbtq', 0.6519015431404114),\n",
       " ('transgendered', 0.6488721966743469),\n",
       " ('lesbians', 0.6255646347999573),\n",
       " ('advocacy', 0.6097405552864075),\n",
       " ('feminist', 0.568495512008667)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.most_similar('lgbt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fdac0-836f-485d-a6bc-c6e487ff7016",
   "metadata": {},
   "source": [
    "## **CREATE DOCUMENT VECTORS FROM WORD EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64976a44-3e76-438a-99f9-6378ca516cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "projectenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
