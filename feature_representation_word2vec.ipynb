{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b2681bb-a91a-42da-ba8e-9a999c7f8810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import ast\n",
    "import re\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "#data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "#NLP & ML libraries\n",
    "from gensim.models import Word2Vec\n",
    "from textblob import TextBlob\n",
    "from nltk import FreqDist\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bbe50fc-de36-41b1-993b-1044cc1b8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed so that code output is deterministic\n",
    "random.seed(30)  # Set the seed for Python's random module\n",
    "np.random.seed(30)  # Set the seed for NumPy's random module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7de6faf-08e5-4b9a-9237-4b6f1a2e6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cleaned data\n",
    "\n",
    "def list_converter(text):\n",
    "    #to revert list->str conversion from pd.read_csv\n",
    "    return ast.literal_eval(text)\n",
    "\n",
    "\n",
    "data = pd.read_csv('Data/training_corpus.csv', converters ={'tokens':list_converter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4226a4d3-22a8-4ebe-b077-575d16a727a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99186, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfo2hl</td>\n",
       "      <td>2021</td>\n",
       "      <td>*Cuntry roads, take me hoem*</td>\n",
       "      <td>cuntry roads hoem</td>\n",
       "      <td>3</td>\n",
       "      <td>[cuntry, road, hoem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfqkbv</td>\n",
       "      <td>2021</td>\n",
       "      <td>That’s been there for several years, sent a pi...</td>\n",
       "      <td>years sent pic cuntry friend long time ago</td>\n",
       "      <td>8</td>\n",
       "      <td>[year, send, pic, cuntry, friend, long, time, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfou07</td>\n",
       "      <td>2021</td>\n",
       "      <td>I am single and I have not traveled to any cun...</td>\n",
       "      <td>single traveled cuntry past year</td>\n",
       "      <td>5</td>\n",
       "      <td>[single, travel, cuntry, past, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfrgpe</td>\n",
       "      <td>2021</td>\n",
       "      <td>What happens when you shop at dragon mart...</td>\n",
       "      <td>happens shop dragon mart</td>\n",
       "      <td>4</td>\n",
       "      <td>[happen, shop, dragon, mart]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>gthiiwi</td>\n",
       "      <td>2021</td>\n",
       "      <td>That’s just absolutely hilarious, is this in t...</td>\n",
       "      <td>absolutely hilarious springs souk</td>\n",
       "      <td>4</td>\n",
       "      <td>[absolutely, hilarious, spring, souk]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text_type       ID  year                                          long_text  \\\n",
       "0   comment  gtfo2hl  2021                       *Cuntry roads, take me hoem*   \n",
       "1   comment  gtfqkbv  2021  That’s been there for several years, sent a pi...   \n",
       "2   comment  gtfou07  2021  I am single and I have not traveled to any cun...   \n",
       "3   comment  gtfrgpe  2021       What happens when you shop at dragon mart...   \n",
       "4   comment  gthiiwi  2021  That’s just absolutely hilarious, is this in t...   \n",
       "\n",
       "                                   clean_text  word_count  \\\n",
       "0                           cuntry roads hoem           3   \n",
       "1  years sent pic cuntry friend long time ago           8   \n",
       "2            single traveled cuntry past year           5   \n",
       "3                    happens shop dragon mart           4   \n",
       "4           absolutely hilarious springs souk           4   \n",
       "\n",
       "                                              tokens  \n",
       "0                               [cuntry, road, hoem]  \n",
       "1  [year, send, pic, cuntry, friend, long, time, ...  \n",
       "2               [single, travel, cuntry, past, year]  \n",
       "3                       [happen, shop, dragon, mart]  \n",
       "4              [absolutely, hilarious, spring, souk]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns = ['index'])\n",
    "print (data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a90b0-730e-445f-8b85-31be72b739e4",
   "metadata": {},
   "source": [
    "## **Word2Vec Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "684d9375-5fc5-4c94-95b3-fc3fcbbed923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cuntry', 'road', 'hoem'],\n",
       " ['year', 'send', 'pic', 'cuntry', 'friend', 'long', 'time', 'ago'],\n",
       " ['single', 'travel', 'cuntry', 'past', 'year'],\n",
       " ['happen', 'shop', 'dragon', 'mart'],\n",
       " ['absolutely', 'hilarious', 'spring', 'souk']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert df['tokens'] to list of strings for word2vec model\n",
    "corpus = data['tokens'].tolist()#.apply(lambda token: ' '.join(token)).tolist()\n",
    "\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b45ea010-a712-4035-8952-fc3fbef3a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time() #track start time of execution\n",
    "\n",
    "#code from codebasics and semicolon, both on youtube\n",
    "\n",
    "#instantiate word2vec model\n",
    "word2vec = Word2Vec (window = 5, min_count = 2, workers = 4)\n",
    "   \n",
    "#build a vocabulary\n",
    "word2vec.build_vocab(corpus, progress_per = 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d745b77-a89b-403c-b2b7-fc4c185005c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99186"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da988379-baa1-4cf0-b11c-622c29f68051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6609977, 7045680)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the word2vec model\n",
    "\n",
    "word2vec.train(\n",
    "                corpus,\n",
    "                total_examples = word2vec.corpus_count,\n",
    "                epochs = word2vec.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6fa63580-bc00-499d-a96d-e308cb515b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('waitrose', 0.9512330293655396),\n",
       " ('nesto', 0.9506546258926392),\n",
       " ('vegetable', 0.9461355805397034),\n",
       " ('spinneys', 0.9418869614601135),\n",
       " ('cafeteria', 0.9361352324485779),\n",
       " ('garlic', 0.935577929019928),\n",
       " ('spinney', 0.9343827962875366),\n",
       " ('carton', 0.9331820607185364),\n",
       " ('veggie', 0.9312180280685425),\n",
       " ('biryani', 0.9304446578025818)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar ('lulu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574248b-ba20-490a-b128-02740445be03",
   "metadata": {},
   "source": [
    "## **Word2Vec Model** with custom stopwords removed from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "33a41365-01da-4fc2-91ec-3ddf56de1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#created the filtered corpus from another notebook and imported it here for use\n",
    "# Read the file\n",
    "with open('updated_corpus.txt', 'r') as f:\n",
    "    updated_data = f.readlines()\n",
    "\n",
    "# Remove newline characters\n",
    "updated_data = [line.strip() for line in updated_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "399a79f4-e118-49c1-b632-dcbe3f1be931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated_data is a <class 'list'>, and the first five elements are:\n",
      " ['road', '', 'single travel past', 'shop dragon mart', 'hilarious spring souk']\n"
     ]
    }
   ],
   "source": [
    "print (f'updated_data is a {type(updated_data)}, and the first five elements are:\\n',updated_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4bc3f6c1-c4ee-46a1-b9ce-3a4a91ecb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list of strings to dataframe column with list of tokens on each row\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "updated_data = pd.DataFrame(updated_data, columns = ['tokens'])\n",
    "updated_data = updated_data['tokens'].apply(lambda x: list(tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e6755b26-876d-42e5-b542-efc76618abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                       [road]\n",
       "1                           []\n",
       "2       [single, travel, past]\n",
       "3         [shop, dragon, mart]\n",
       "4    [hilarious, spring, souk]\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(updated_data))\n",
    "updated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3fd6e35d-4d81-4e71-8998-c74b78511486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['road'],\n",
       " [],\n",
       " ['single', 'travel', 'past'],\n",
       " ['shop', 'dragon', 'mart'],\n",
       " ['hilarious', 'spring', 'souk']]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert ['tokens'] column to list of strings for word2vec model\n",
    "updated_corpus = updated_data.tolist()#.apply(lambda token: ' '.join(token)).tolist()\n",
    "\n",
    "updated_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "092febb8-13a9-421b-925f-e97b8966ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time() #track start time of execution\n",
    "\n",
    "#code from codebasics and semicolon, both on youtube\n",
    "\n",
    "#instantiate word2vec model\n",
    "word2vec2 = Word2Vec (window = 5, min_count = 2, workers = 4)\n",
    "   \n",
    "#build a vocabulary\n",
    "word2vec2.build_vocab(corpus, progress_per = 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bda49e32-129d-4283-b9e9-05fb172e724f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99186"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of sentences in vocabulary\n",
    "word2vec2.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7146b649-752a-47fd-8a0a-f7e15709de0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4198495, 4198495)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the word2vec model\n",
    "\n",
    "word2vec2.train(\n",
    "                updated_corpus,\n",
    "                total_examples = word2vec2.corpus_count,\n",
    "                epochs = word2vec2.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e684cb44-9a8d-449d-9f9a-f6a2f41be75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hypermarket', 0.9551829099655151),\n",
       " ('waitrose', 0.9474326968193054),\n",
       " ('nesto', 0.9464415907859802),\n",
       " ('spinney', 0.9345912933349609),\n",
       " ('carrefour', 0.9311429858207703),\n",
       " ('spinneys', 0.919035792350769),\n",
       " ('viva', 0.9174704551696777),\n",
       " ('cafeteria', 0.9123212099075317),\n",
       " ('mcdonalds', 0.9016396999359131),\n",
       " ('kg', 0.8944598436355591)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec2.wv.most_similar ('lulu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80568523-7529-484b-bca3-681424544c95",
   "metadata": {},
   "source": [
    "## **Word2Vec with bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d7193727-a947-4bf0-8d34-2404047b1fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases<816198 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "#first model on full corpus\n",
    "\n",
    "#code from gensim documentation\n",
    "from gensim.models.phrases import Phrases, \n",
    "\n",
    "#train bigram detector\n",
    "bigram_transform = Phrases (corpus, min_count = 1) #default threshold is 10.0, fewer phrases will be created\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "be3aceb2-7f4a-4d99-a8b6-90d07d67b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anymore', 'new', 'labor_law', 'common', 'blatant', 'good', 'company', 'care', 'sale', 'promotional', 'bdm', 'post', 'hire', 'look', 'filter', 'photo', 'cv', 'rife', 'real_estate', 'industry', 'say', 'uae', 'young', 'country', 'learn', 'motivation', 'law', 'change', 'align', 'progressive', 'practice', 'ahead', 'way', 'nature', 'place', 'live']\n"
     ]
    }
   ],
   "source": [
    "#check performance of bigram transform model\n",
    "new_sentence = corpus[330]\n",
    "print(bigram_transform[new_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e64e8cca-b521-4c74-b44e-d5e681a6b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply bi-gram transform on corpus for word2vec model\n",
    "\n",
    "bigram_corpus = [bigram_transform[sentence] for sentence in corpus]\n",
    "\n",
    "bigram_word2vec = Word2Vec (bigram_corpus,window = 5, min_count = 2, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5f21cd84-4fc9-45a9-b6eb-0f464d6731ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hatred', 0.9805039763450623),\n",
       " ('agenda', 0.9785802960395813),\n",
       " ('disrespect', 0.977561354637146),\n",
       " ('ignorance', 0.9768002033233643),\n",
       " ('prejudice', 0.9764350056648254),\n",
       " ('propaganda', 0.9760943055152893),\n",
       " ('lgbtq', 0.9748927354812622),\n",
       " ('homosexuality', 0.9736032485961914),\n",
       " ('sexual', 0.9727028608322144),\n",
       " ('extremist', 0.9723531007766724)]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_word2vec.wv.most_similar('lgbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1dd6ef11-afae-4e9d-a41b-760599b2ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second model on filtered corpus --- additonal stopwords removed (informal conversation common words)\n",
    "\n",
    "#train bigram detector\n",
    "bigram_transform2 = Phrases (updated_corpus, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "468807c3-66c1-4e2e-821a-588c1a4a2069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anymore', 'new', 'labor_law', 'common', 'blatant', 'good', 'company', 'care', 'sale', 'promotional', 'bdm', 'post', 'hire', 'look', 'filter', 'photo', 'cv', 'rife', 'real_estate', 'industry', 'say', 'uae', 'young', 'country', 'learn', 'motivation', 'law', 'change', 'align', 'progressive', 'practice', 'ahead', 'way', 'nature', 'place', 'live']\n"
     ]
    }
   ],
   "source": [
    "#check performance of bigram transform model\n",
    "new_sentence = corpus[330]\n",
    "print(bigram_transform[new_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "430d0634-ec61-4051-967d-f9a8ffb2f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply bi-gram transform on corpus for word2vec model\n",
    "\n",
    "bigram_corpus2 = [bigram_transform2[sentence] for sentence in corpus]\n",
    "\n",
    "bigram_word2vec2 = Word2Vec (bigram_corpus2,window = 5, min_count = 2, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "fd7ff264-f4a1-4ca0-a591-a50d03257c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brainwash', 0.984478771686554),\n",
       " ('lgbtq', 0.9838911890983582),\n",
       " ('tolerate', 0.9793130159378052),\n",
       " ('homosexuality', 0.978199303150177),\n",
       " ('uneducated', 0.9781699180603027),\n",
       " ('hypocrite', 0.9765817523002625),\n",
       " ('universe', 0.9762008786201477),\n",
       " ('hatred', 0.9754001498222351),\n",
       " ('intolerant', 0.9738975167274475),\n",
       " ('moron', 0.9735695719718933)]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_word2vec2.wv.most_similar('lgbt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386f184-a063-4cc1-bf80-73065f3c0a2c",
   "metadata": {},
   "source": [
    "## **CLUSTERING WITH THE WORD EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fdac0-836f-485d-a6bc-c6e487ff7016",
   "metadata": {},
   "source": [
    "## **CREATE DOCUMENT VECTORS FROM WORD EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64976a44-3e76-438a-99f9-6378ca516cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "projectenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
