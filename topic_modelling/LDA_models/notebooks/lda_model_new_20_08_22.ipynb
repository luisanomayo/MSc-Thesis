{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a695a6-2a99-4cb6-93aa-155a55358ce0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h1><b>Topic Modelling with LDA:</b></h1>\n",
    "    <p style = \"color: black\"> Using LDA model on pre-processed data. Stop word removal during pre-processing is limited only to common english stop words.<br>Any further stop word removal will be done when creating bag of words with the gensim library \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6752d-08b0-4c10-8834-b74f45ead103",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df68e386-5c9a-4f11-b16d-d46f50155a83",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h2><b>I. Setting Up Environment</b></h2>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fdce37f-9850-4e32-8b6f-851fdcd196ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing libraries\n",
    "\n",
    "import re #regular expressions library for text manipulation\n",
    "import string\n",
    "import unicodedata\n",
    "import itertools\n",
    "from autocorrect import Speller\n",
    "import emoji\n",
    "import contractions\n",
    "\n",
    "#data transformation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import csv\n",
    "\n",
    "#NLP specific libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "import spacymoji\n",
    "\n",
    "\n",
    "#topic modelling libraries\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel, LdaMulticore\n",
    "\n",
    "\n",
    "#for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "#others\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2d063-b4bd-4693-93aa-9e91745d67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional nlp models\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabf5b6e-4696-4005-a47a-94646b38158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed so that code output is deterministic\n",
    "random.seed(200)  # Set the seed for Python's random module\n",
    "np.random.seed(200)  # Set the seed for NumPy's random module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b37cf-edac-4e80-8a1e-b8efd410af75",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac894df-4b48-4336-b83b-1a0998e3d8f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h2><b>II. Import Data into DataFrame</b></h2>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0bdd9eb-eb7f-471a-85f7-d9487e20ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments (filepath):\n",
    "    \"\"\"\n",
    "    import comments from csv file into pandas dataframe,\n",
    "    \n",
    "    and carry out initial cleaning including removing deleted comments,\n",
    "    \n",
    "    correcting datetime data type, remove unwanted columns like `isSubmitter`,\n",
    "    \n",
    "    remove duplicates, etc\n",
    "    \n",
    "    \"\"\"\n",
    "    #import data\n",
    "    df = pd.read_csv(filepath, low_memory = False)\n",
    "    \n",
    "    #remove deleted comments if any\n",
    "    df = df[df.Body != '[deleted]']\n",
    "    df = df[df.Body != '[removed]']\n",
    "    df = df.dropna(subset=['Body'])\n",
    "    \n",
    "    #remove comments with missing id\n",
    "    drop_index = df[df.isSubmitter.isnull()].index\n",
    "    df.drop(drop_index, inplace = True)\n",
    "    \n",
    "    #remove duplicates if any\n",
    "    df = df.drop_duplicates(subset =['ID'], ignore_index = True)\n",
    "    df.reset_index (drop = True, inplace = True)\n",
    "    \n",
    "    #correct data types and column label\n",
    "    df['Date_Created'] = pd.to_datetime(df['Date_Created'])\n",
    "    df['year'] = df['Date_Created'].dt.year\n",
    "    df['Score'] = df['Score'].astype('int') \n",
    "    df.rename(columns = {'Author_ID': \"Author\"}, inplace = True) \n",
    "    \n",
    "    #rename 'Body' column to text\n",
    "    df.rename(columns = {'Body': 'long_text',\n",
    "                        'Date_Created': 'date_created'}, inplace = True)\n",
    "    \n",
    "    #remove unnecessary columns\n",
    "    df.drop(columns = ['Unnamed: 0', 'Author', 'Score',\n",
    "       'Parent_ID', 'Submission_ID', 'Subreddit', 'isParent', 'isSubmitter'], inplace = True)\n",
    "    \n",
    "    #remove any extra whitespace in column labels\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    #include column to denote row is comment entry\n",
    "    df['text_type'] = 'comment'\n",
    "    \n",
    "    #rearrange column order\n",
    "    df = df[['text_type','ID','date_created', 'year', 'long_text']]\n",
    "        \n",
    "    return df\n",
    "\n",
    "def clean_submissions(filepath):\n",
    "    \"\"\"\n",
    "    import submissions/posts from csv file into pandas dataframe,\n",
    "    \n",
    "    and carry out initial cleaning including removing deleted comments,\n",
    "    \n",
    "    correcting datetime data type, remove unwanted columns like `Unnamed`,\n",
    "    \n",
    "    remove duplicates, etc\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, low_memory = False)\n",
    "    \n",
    "    #drop duplicate posts\n",
    "    df = df.drop_duplicates(subset =['ID'], ignore_index = True)\n",
    "    df.reset_index (drop = True, inplace = True)\n",
    "    \n",
    "    #create column for post title + post text\n",
    "    df['long_text'] = df['Title']+ \" \" +df['Post Text'].fillna('')\n",
    "    \n",
    "    #adjust data types\n",
    "    df['Date Created'] = pd.to_datetime(df['Date Created'])\n",
    "    df['year'] = df['year'].astype('int')\n",
    "    \n",
    "    #rename columns\n",
    "    df.rename(columns = {'Date Created': 'date_created'}, inplace = True)\n",
    "    \n",
    "    #remove unwanted columns\n",
    "    df = df.drop(columns = ['Unnamed: 0', 'Title','Post Text', 'Score',\n",
    "       'Total Comments', 'Post URL', 'SubReddit','Unnamed: 0.1'])\n",
    "    \n",
    "    #include column to denote row is comment entry\n",
    "    df['text_type'] = 'submission'\n",
    "    \n",
    "    #reorder columns - 'ID', 'Post Text'\n",
    "    df  = df[['text_type','ID', 'date_created','year', 'long_text']]\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4ed964-bfc8-42da-84e9-467bf55f0bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../Data/subset_sample_no_label.csv',\n",
       " '../../../Data/filtered_corpus.csv',\n",
       " '../../../Data/full_posts.csv',\n",
       " '../../../Data/full_data_no_preprocessing.csv',\n",
       " '../../../Data/training_data.csv',\n",
       " '../../../Data/lda_train.csv',\n",
       " '../../../Data/vocabulary.csv',\n",
       " '../../../Data/emoji_subset.csv',\n",
       " '../../../Data/sampled_subset.csv',\n",
       " '../../../Data/subs_topics.csv',\n",
       " '../../../Data/comments.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access dataset files\n",
    "folder_path = os.path.join(\"..\",\"..\", \"..\", \"Data\")  # Adjust the path accordingly\n",
    "file_type = \"*.csv\"\n",
    "\n",
    "# List of dataset file paths\n",
    "document_path = glob(os.path.join(folder_path, file_type))\n",
    "\n",
    "document_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e58718-f91f-4b1a-8bbc-77908068b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_filepath = '' #file path for csv file of scraped comments \n",
    "submissions_filepath = '' #file path for csv file of scrapped submissions\n",
    "\n",
    "for path in document_path:\n",
    "    if \"comments\" in path:\n",
    "        comments_filepath = comments_filepath + path\n",
    "    elif \"full_posts\" in path:\n",
    "        submissions_filepath = submissions_filepath + path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af91719f-d0c8-48d4-9568-a5b6c21981f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../Data/comments.csv ../../../Data/full_posts.csv\n"
     ]
    }
   ],
   "source": [
    "print (comments_filepath, submissions_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203c5107-b995-4491-b345-76472ef96aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfo2hl</td>\n",
       "      <td>2021-04-05 13:00:32</td>\n",
       "      <td>2021</td>\n",
       "      <td>*Cuntry roads, take me hoem*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfqkbv</td>\n",
       "      <td>2021-04-05 13:41:40</td>\n",
       "      <td>2021</td>\n",
       "      <td>That’s been there for several years, sent a pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfou07</td>\n",
       "      <td>2021-04-05 13:13:23</td>\n",
       "      <td>2021</td>\n",
       "      <td>I am single and I have not traveled to any cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfrgpe</td>\n",
       "      <td>2021-04-05 13:56:09</td>\n",
       "      <td>2021</td>\n",
       "      <td>What happens when you shop at dragon mart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtg5mwv</td>\n",
       "      <td>2021-04-05 16:51:54</td>\n",
       "      <td>2021</td>\n",
       "      <td>I am cunting on them to do so 😅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128878</th>\n",
       "      <td>submission</td>\n",
       "      <td>14f46ji</td>\n",
       "      <td>2023-06-21 14:40:54</td>\n",
       "      <td>2023</td>\n",
       "      <td>Best beauty saloons in Dubai? Hello fellas, I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128879</th>\n",
       "      <td>submission</td>\n",
       "      <td>14f4uyi</td>\n",
       "      <td>2023-06-21 15:15:27</td>\n",
       "      <td>2023</td>\n",
       "      <td>Found the r/dubai redditors who kept telling m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128880</th>\n",
       "      <td>submission</td>\n",
       "      <td>14f4ri3</td>\n",
       "      <td>2023-06-21 15:10:25</td>\n",
       "      <td>2023</td>\n",
       "      <td>Scam ? Healthy.line My sister has a CBD debit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128881</th>\n",
       "      <td>submission</td>\n",
       "      <td>14f4k3r</td>\n",
       "      <td>2023-06-21 15:00:34</td>\n",
       "      <td>2023</td>\n",
       "      <td>Thoughts on Expo City properties? Anyone else ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128882</th>\n",
       "      <td>submission</td>\n",
       "      <td>14f8d30</td>\n",
       "      <td>2023-06-21 17:53:14</td>\n",
       "      <td>2023</td>\n",
       "      <td>What to do when the neighbour parks like this?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128883 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         text_type       ID        date_created  year  \\\n",
       "0          comment  gtfo2hl 2021-04-05 13:00:32  2021   \n",
       "1          comment  gtfqkbv 2021-04-05 13:41:40  2021   \n",
       "2          comment  gtfou07 2021-04-05 13:13:23  2021   \n",
       "3          comment  gtfrgpe 2021-04-05 13:56:09  2021   \n",
       "4          comment  gtg5mwv 2021-04-05 16:51:54  2021   \n",
       "...            ...      ...                 ...   ...   \n",
       "128878  submission  14f46ji 2023-06-21 14:40:54  2023   \n",
       "128879  submission  14f4uyi 2023-06-21 15:15:27  2023   \n",
       "128880  submission  14f4ri3 2023-06-21 15:10:25  2023   \n",
       "128881  submission  14f4k3r 2023-06-21 15:00:34  2023   \n",
       "128882  submission  14f8d30 2023-06-21 17:53:14  2023   \n",
       "\n",
       "                                                long_text  \n",
       "0                            *Cuntry roads, take me hoem*  \n",
       "1       That’s been there for several years, sent a pi...  \n",
       "2       I am single and I have not traveled to any cun...  \n",
       "3            What happens when you shop at dragon mart...  \n",
       "4                         I am cunting on them to do so 😅  \n",
       "...                                                   ...  \n",
       "128878  Best beauty saloons in Dubai? Hello fellas, I ...  \n",
       "128879  Found the r/dubai redditors who kept telling m...  \n",
       "128880  Scam ? Healthy.line My sister has a CBD debit ...  \n",
       "128881  Thoughts on Expo City properties? Anyone else ...  \n",
       "128882  What to do when the neighbour parks like this?...  \n",
       "\n",
       "[128883 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([clean_comments(comments_filepath), \n",
    "                  clean_submissions(submissions_filepath)], \n",
    "                 ignore_index = True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d117253-4f49-4eff-8f67-76a0082b7bb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h2><b>III. Text Preprocessing</b></h2>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa4d85b-f5a6-4fda-b60f-11677f061d61",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>a. convert text to lowercase</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0154efa8-75b1-4736-b133-1cf18e202985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52407</th>\n",
       "      <td>comment</td>\n",
       "      <td>jby7q9s</td>\n",
       "      <td>2023-03-12 21:20:25</td>\n",
       "      <td>2023</td>\n",
       "      <td>If I’m on SZR doing 140 (and I’m on there ofte...</td>\n",
       "      <td>if i’m on szr doing 140 (and i’m on there ofte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91135</th>\n",
       "      <td>comment</td>\n",
       "      <td>jo3w5e6</td>\n",
       "      <td>2023-06-14 18:40:37</td>\n",
       "      <td>2023</td>\n",
       "      <td>So forcing LGBTQ down peoples throat is ok wit...</td>\n",
       "      <td>so forcing lgbtq down peoples throat is ok wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18066</th>\n",
       "      <td>comment</td>\n",
       "      <td>g57s994</td>\n",
       "      <td>2020-09-14 13:38:27</td>\n",
       "      <td>2020</td>\n",
       "      <td>Sorry mate. I only bought 1 pack. It was a mix...</td>\n",
       "      <td>sorry mate. i only bought 1 pack. it was a mix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34113</th>\n",
       "      <td>comment</td>\n",
       "      <td>hkgr4lz</td>\n",
       "      <td>2021-11-13 19:26:35</td>\n",
       "      <td>2021</td>\n",
       "      <td>Not sure any consulting companies will look at...</td>\n",
       "      <td>not sure any consulting companies will look at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24513</th>\n",
       "      <td>comment</td>\n",
       "      <td>esoqfqw</td>\n",
       "      <td>2019-07-03 20:48:42</td>\n",
       "      <td>2019</td>\n",
       "      <td>Yuck</td>\n",
       "      <td>yuck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_type       ID        date_created  year  \\\n",
       "52407   comment  jby7q9s 2023-03-12 21:20:25  2023   \n",
       "91135   comment  jo3w5e6 2023-06-14 18:40:37  2023   \n",
       "18066   comment  g57s994 2020-09-14 13:38:27  2020   \n",
       "34113   comment  hkgr4lz 2021-11-13 19:26:35  2021   \n",
       "24513   comment  esoqfqw 2019-07-03 20:48:42  2019   \n",
       "\n",
       "                                               long_text  \\\n",
       "52407  If I’m on SZR doing 140 (and I’m on there ofte...   \n",
       "91135  So forcing LGBTQ down peoples throat is ok wit...   \n",
       "18066  Sorry mate. I only bought 1 pack. It was a mix...   \n",
       "34113  Not sure any consulting companies will look at...   \n",
       "24513                                               Yuck   \n",
       "\n",
       "                                              clean_text  \n",
       "52407  if i’m on szr doing 140 (and i’m on there ofte...  \n",
       "91135  so forcing lgbtq down peoples throat is ok wit...  \n",
       "18066  sorry mate. i only bought 1 pack. it was a mix...  \n",
       "34113  not sure any consulting companies will look at...  \n",
       "24513                                               yuck  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text'] = data['long_text'].apply(lambda text: text.lower())\n",
    "\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa3ea4-aefb-4851-9963-25bcb891e25a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>b. expand word contractions</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2adc791-a998-4032-b4b9-bdd376f65695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72442</th>\n",
       "      <td>comment</td>\n",
       "      <td>gd2xczj</td>\n",
       "      <td>2020-11-21 21:29:32</td>\n",
       "      <td>2020</td>\n",
       "      <td>Pwnd</td>\n",
       "      <td>pwnd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81671</th>\n",
       "      <td>comment</td>\n",
       "      <td>ji25q9t</td>\n",
       "      <td>2023-04-28 18:42:20</td>\n",
       "      <td>2023</td>\n",
       "      <td>If a traffic violation can be excused, so can ...</td>\n",
       "      <td>if a traffic violation can be excused, so can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12803</th>\n",
       "      <td>comment</td>\n",
       "      <td>i0uv3pr</td>\n",
       "      <td>2022-03-16 11:08:03</td>\n",
       "      <td>2022</td>\n",
       "      <td>Is this near wafi?</td>\n",
       "      <td>is this near wafi?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>comment</td>\n",
       "      <td>j8ows67</td>\n",
       "      <td>2023-02-16 02:07:49</td>\n",
       "      <td>2023</td>\n",
       "      <td>Oh! Got it</td>\n",
       "      <td>oh! got it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60256</th>\n",
       "      <td>comment</td>\n",
       "      <td>i8mqltf</td>\n",
       "      <td>2022-05-15 03:01:12</td>\n",
       "      <td>2022</td>\n",
       "      <td>What's the situation with noon food, I've been...</td>\n",
       "      <td>what is the situation with noon food, i have b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_type       ID        date_created  year  \\\n",
       "72442   comment  gd2xczj 2020-11-21 21:29:32  2020   \n",
       "81671   comment  ji25q9t 2023-04-28 18:42:20  2023   \n",
       "12803   comment  i0uv3pr 2022-03-16 11:08:03  2022   \n",
       "5790    comment  j8ows67 2023-02-16 02:07:49  2023   \n",
       "60256   comment  i8mqltf 2022-05-15 03:01:12  2022   \n",
       "\n",
       "                                               long_text  \\\n",
       "72442                                               Pwnd   \n",
       "81671  If a traffic violation can be excused, so can ...   \n",
       "12803                                 Is this near wafi?   \n",
       "5790                                          Oh! Got it   \n",
       "60256  What's the situation with noon food, I've been...   \n",
       "\n",
       "                                              clean_text  \n",
       "72442                                               pwnd  \n",
       "81671  if a traffic violation can be excused, so can ...  \n",
       "12803                                 is this near wafi?  \n",
       "5790                                          oh! got it  \n",
       "60256  what is the situation with noon food, i have b...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(lambda text: contractions.fix(text)) \n",
    "\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c151e-1e2a-41b3-870a-a149a963d687",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>c. remove URLs</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71417843-cf72-4260-8036-8900f14a07c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtgyqzz</td>\n",
       "      <td>2021-04-05 20:49:12</td>\n",
       "      <td>2021</td>\n",
       "      <td>Meditation And Relaxation Music https://youtu....</td>\n",
       "      <td>meditation and relaxation music https://youtu....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>comment</td>\n",
       "      <td>gpqwzwc</td>\n",
       "      <td>2021-03-05 10:34:53</td>\n",
       "      <td>2021</td>\n",
       "      <td>I am curious about this Apollo Fintech GSX coi...</td>\n",
       "      <td>i am curious about this apollo fintech gsx coi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>comment</td>\n",
       "      <td>gpqwrek</td>\n",
       "      <td>2021-03-05 10:31:45</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://www.instagram.com/p/CKhzbwpH0c4/?igshi...</td>\n",
       "      <td>https://www.instagram.com/p/ckhzbwph0c4/?igshi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>comment</td>\n",
       "      <td>gq3tah4</td>\n",
       "      <td>2021-03-07 19:49:41</td>\n",
       "      <td>2021</td>\n",
       "      <td>With all due respect, do you expect a law enfo...</td>\n",
       "      <td>with all due respect, do you expect a law enfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>comment</td>\n",
       "      <td>jb5kpz4</td>\n",
       "      <td>2023-03-06 20:35:11</td>\n",
       "      <td>2023</td>\n",
       "      <td>Here: Dubai Festival City\\nhttps://maps.app.go...</td>\n",
       "      <td>here: dubai festival city\\nhttps://maps.app.go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text_type       ID        date_created  year  \\\n",
       "17    comment  gtgyqzz 2021-04-05 20:49:12  2021   \n",
       "83    comment  gpqwzwc 2021-03-05 10:34:53  2021   \n",
       "93    comment  gpqwrek 2021-03-05 10:31:45  2021   \n",
       "171   comment  gq3tah4 2021-03-07 19:49:41  2021   \n",
       "290   comment  jb5kpz4 2023-03-06 20:35:11  2023   \n",
       "\n",
       "                                             long_text  \\\n",
       "17   Meditation And Relaxation Music https://youtu....   \n",
       "83   I am curious about this Apollo Fintech GSX coi...   \n",
       "93   https://www.instagram.com/p/CKhzbwpH0c4/?igshi...   \n",
       "171  With all due respect, do you expect a law enfo...   \n",
       "290  Here: Dubai Festival City\\nhttps://maps.app.go...   \n",
       "\n",
       "                                            clean_text  \n",
       "17   meditation and relaxation music https://youtu....  \n",
       "83   i am curious about this apollo fintech gsx coi...  \n",
       "93   https://www.instagram.com/p/ckhzbwph0c4/?igshi...  \n",
       "171  with all due respect, do you expect a law enfo...  \n",
       "290  here: dubai festival city\\nhttps://maps.app.go...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index of rows with urls\n",
    "html_index = data[data['long_text'].str.contains(\"https\")].index\n",
    "data.loc[html_index].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1344f04-2e39-4759-88c6-d32619d04189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtgyqzz</td>\n",
       "      <td>2021-04-05 20:49:12</td>\n",
       "      <td>2021</td>\n",
       "      <td>Meditation And Relaxation Music https://youtu....</td>\n",
       "      <td>meditation and relaxation music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>comment</td>\n",
       "      <td>gpqwzwc</td>\n",
       "      <td>2021-03-05 10:34:53</td>\n",
       "      <td>2021</td>\n",
       "      <td>I am curious about this Apollo Fintech GSX coi...</td>\n",
       "      <td>i am curious about this apollo fintech gsx coi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>comment</td>\n",
       "      <td>gpqwrek</td>\n",
       "      <td>2021-03-05 10:31:45</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://www.instagram.com/p/CKhzbwpH0c4/?igshi...</td>\n",
       "      <td>\\n\\nhere is a video explaining the same.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>comment</td>\n",
       "      <td>gq3tah4</td>\n",
       "      <td>2021-03-07 19:49:41</td>\n",
       "      <td>2021</td>\n",
       "      <td>With all due respect, do you expect a law enfo...</td>\n",
       "      <td>with all due respect, do you expect a law enfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>comment</td>\n",
       "      <td>jb5kpz4</td>\n",
       "      <td>2023-03-06 20:35:11</td>\n",
       "      <td>2023</td>\n",
       "      <td>Here: Dubai Festival City\\nhttps://maps.app.go...</td>\n",
       "      <td>here: dubai festival city\\n \\n\\nthen follow th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text_type       ID        date_created  year  \\\n",
       "17    comment  gtgyqzz 2021-04-05 20:49:12  2021   \n",
       "83    comment  gpqwzwc 2021-03-05 10:34:53  2021   \n",
       "93    comment  gpqwrek 2021-03-05 10:31:45  2021   \n",
       "171   comment  gq3tah4 2021-03-07 19:49:41  2021   \n",
       "290   comment  jb5kpz4 2023-03-06 20:35:11  2023   \n",
       "\n",
       "                                             long_text  \\\n",
       "17   Meditation And Relaxation Music https://youtu....   \n",
       "83   I am curious about this Apollo Fintech GSX coi...   \n",
       "93   https://www.instagram.com/p/CKhzbwpH0c4/?igshi...   \n",
       "171  With all due respect, do you expect a law enfo...   \n",
       "290  Here: Dubai Festival City\\nhttps://maps.app.go...   \n",
       "\n",
       "                                            clean_text  \n",
       "17                   meditation and relaxation music    \n",
       "83   i am curious about this apollo fintech gsx coi...  \n",
       "93            \\n\\nhere is a video explaining the same.  \n",
       "171  with all due respect, do you expect a law enfo...  \n",
       "290  here: dubai festival city\\n \\n\\nthen follow th...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regex pattern for urls\n",
    "url_pattern = r'https?://\\S+'\n",
    "#replace url with empty string\n",
    "data['clean_text'] = data['clean_text'].apply(lambda text: re.sub(url_pattern, ' ', text, flags=re.MULTILINE))\n",
    "\n",
    "data.loc[html_index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916698f5-9fc9-43e3-b64a-c9f5dc9371a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>d. remove accents from characters</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1af6084-b233-4214-85a9-74c7cb3ac67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20846</th>\n",
       "      <td>comment</td>\n",
       "      <td>gekzqi7</td>\n",
       "      <td>2020-12-04 14:46:42</td>\n",
       "      <td>2020</td>\n",
       "      <td>They’re keeping a 2m distance horizontally but...</td>\n",
       "      <td>they are keeping a 2m distance horizontally bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90704</th>\n",
       "      <td>comment</td>\n",
       "      <td>jolw1gi</td>\n",
       "      <td>2023-06-18 21:40:26</td>\n",
       "      <td>2023</td>\n",
       "      <td>The indoor sports thing at Dubai World Trade c...</td>\n",
       "      <td>the indoor sports thing at dubai world trade c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>comment</td>\n",
       "      <td>j8mroan</td>\n",
       "      <td>2023-02-15 17:37:07</td>\n",
       "      <td>2023</td>\n",
       "      <td>Confirmed; it’s true. Just need to have the ac...</td>\n",
       "      <td>confirmed; it is true. just need to have the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76136</th>\n",
       "      <td>comment</td>\n",
       "      <td>gjx3t2n</td>\n",
       "      <td>2021-01-20 10:19:43</td>\n",
       "      <td>2021</td>\n",
       "      <td>Wait a minute....mask? handwashing, social dis...</td>\n",
       "      <td>wait a minute....mask? handwashing, social dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>comment</td>\n",
       "      <td>gdo37cv</td>\n",
       "      <td>2020-11-26 19:15:24</td>\n",
       "      <td>2020</td>\n",
       "      <td>But I heard girls like bad boys &amp; I’m bad at e...</td>\n",
       "      <td>but i heard girls like bad boys &amp; i am bad at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_type       ID        date_created  year  \\\n",
       "20846   comment  gekzqi7 2020-12-04 14:46:42  2020   \n",
       "90704   comment  jolw1gi 2023-06-18 21:40:26  2023   \n",
       "5786    comment  j8mroan 2023-02-15 17:37:07  2023   \n",
       "76136   comment  gjx3t2n 2021-01-20 10:19:43  2021   \n",
       "2828    comment  gdo37cv 2020-11-26 19:15:24  2020   \n",
       "\n",
       "                                               long_text  \\\n",
       "20846  They’re keeping a 2m distance horizontally but...   \n",
       "90704  The indoor sports thing at Dubai World Trade c...   \n",
       "5786   Confirmed; it’s true. Just need to have the ac...   \n",
       "76136  Wait a minute....mask? handwashing, social dis...   \n",
       "2828   But I heard girls like bad boys & I’m bad at e...   \n",
       "\n",
       "                                              clean_text  \n",
       "20846  they are keeping a 2m distance horizontally bu...  \n",
       "90704  the indoor sports thing at dubai world trade c...  \n",
       "5786   confirmed; it is true. just need to have the a...  \n",
       "76136  wait a minute....mask? handwashing, social dis...  \n",
       "2828   but i heard girls like bad boys & i am bad at ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(lambda text: \n",
    "                                              unicodedata.normalize('NFKD', text).\n",
    "                                              encode('ASCII', 'ignore').decode('utf-8'))\n",
    "\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65038a-27df-4258-8fe9-605d1f4a993f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>e. remove punctuations</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0100ec2e-76b2-41b3-a55e-828e035fcd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124628, 125097, 126727, 126869, 127216]\n"
     ]
    }
   ],
   "source": [
    "#index of some rows with punctuations\n",
    "checker_list = ['ifquow','gzl2ec','147gsfl','vtelex',\n",
    " '12pqx6m','fuxrd2','2ui6wu','l4gz0u','14f4uyi','14f8d30']\n",
    "\n",
    "rows_to_check = data[data['ID'].isin(checker_list)].index.tolist()\n",
    "\n",
    "rows_to_check.extend([32003, 116022,18460,5786,30109])\n",
    "\n",
    "rows_to_check.extend(html_index)\n",
    "\n",
    "print(rows_to_check[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1394dc63-71fe-4d2f-8ec0-0b00b1da533b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124628</th>\n",
       "      <td>submission</td>\n",
       "      <td>ifquow</td>\n",
       "      <td>2020-08-24 19:13:26</td>\n",
       "      <td>2020</td>\n",
       "      <td>GUYS WE MADE IT!!! YAY</td>\n",
       "      <td>guys we made it    yay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125097</th>\n",
       "      <td>submission</td>\n",
       "      <td>gzl2ec</td>\n",
       "      <td>2020-06-09 15:11:27</td>\n",
       "      <td>2020</td>\n",
       "      <td>PSA: Immigration to Canada, Australia, NZ Hell...</td>\n",
       "      <td>psa  immigration to canada  australia  nz hell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126727</th>\n",
       "      <td>submission</td>\n",
       "      <td>147gsfl</td>\n",
       "      <td>2023-06-12 10:22:19</td>\n",
       "      <td>2023</td>\n",
       "      <td>How to reach people who are asking for money/g...</td>\n",
       "      <td>how to reach people who are asking for money g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126869</th>\n",
       "      <td>submission</td>\n",
       "      <td>12pqx6m</td>\n",
       "      <td>2023-04-17 22:00:27</td>\n",
       "      <td>2023</td>\n",
       "      <td>How do you plan to spend your Eid holiday? Any...</td>\n",
       "      <td>how do you plan to spend your eid holiday  any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127216</th>\n",
       "      <td>submission</td>\n",
       "      <td>vtelex</td>\n",
       "      <td>2022-07-07 13:32:10</td>\n",
       "      <td>2022</td>\n",
       "      <td>Hi everyone, I'm currently looking for jobs te...</td>\n",
       "      <td>hi everyone  i am currently looking for jobs t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text_type       ID        date_created  year  \\\n",
       "124628  submission   ifquow 2020-08-24 19:13:26  2020   \n",
       "125097  submission   gzl2ec 2020-06-09 15:11:27  2020   \n",
       "126727  submission  147gsfl 2023-06-12 10:22:19  2023   \n",
       "126869  submission  12pqx6m 2023-04-17 22:00:27  2023   \n",
       "127216  submission   vtelex 2022-07-07 13:32:10  2022   \n",
       "\n",
       "                                                long_text  \\\n",
       "124628                            GUYS WE MADE IT!!! YAY    \n",
       "125097  PSA: Immigration to Canada, Australia, NZ Hell...   \n",
       "126727  How to reach people who are asking for money/g...   \n",
       "126869  How do you plan to spend your Eid holiday? Any...   \n",
       "127216  Hi everyone, I'm currently looking for jobs te...   \n",
       "\n",
       "                                               clean_text  \n",
       "124628                            guys we made it    yay   \n",
       "125097  psa  immigration to canada  australia  nz hell...  \n",
       "126727  how to reach people who are asking for money g...  \n",
       "126869  how do you plan to spend your eid holiday  any...  \n",
       "127216  hi everyone  i am currently looking for jobs t...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regex pattern for punctuations\n",
    "punctuation_pattern = r'[^\\w\\s_]'\n",
    "\n",
    "#remove punctuations using `re.sub() method\n",
    "data['clean_text'] = data['clean_text'].apply(lambda text: re.sub(punctuation_pattern, ' ', text))\n",
    "\n",
    "data.iloc[rows_to_check].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f0e72-ffa9-4581-a1de-f8cae4f7187a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>f. remove new line & tab characters</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2ecfff7-a66c-44f2-80bd-0f7fc09264a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124628</th>\n",
       "      <td>submission</td>\n",
       "      <td>ifquow</td>\n",
       "      <td>2020-08-24 19:13:26</td>\n",
       "      <td>2020</td>\n",
       "      <td>GUYS WE MADE IT!!! YAY</td>\n",
       "      <td>guys we made it    yay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125097</th>\n",
       "      <td>submission</td>\n",
       "      <td>gzl2ec</td>\n",
       "      <td>2020-06-09 15:11:27</td>\n",
       "      <td>2020</td>\n",
       "      <td>PSA: Immigration to Canada, Australia, NZ Hell...</td>\n",
       "      <td>psa  immigration to canada  australia  nz hell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126727</th>\n",
       "      <td>submission</td>\n",
       "      <td>147gsfl</td>\n",
       "      <td>2023-06-12 10:22:19</td>\n",
       "      <td>2023</td>\n",
       "      <td>How to reach people who are asking for money/g...</td>\n",
       "      <td>how to reach people who are asking for money g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126869</th>\n",
       "      <td>submission</td>\n",
       "      <td>12pqx6m</td>\n",
       "      <td>2023-04-17 22:00:27</td>\n",
       "      <td>2023</td>\n",
       "      <td>How do you plan to spend your Eid holiday? Any...</td>\n",
       "      <td>how do you plan to spend your eid holiday  any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127216</th>\n",
       "      <td>submission</td>\n",
       "      <td>vtelex</td>\n",
       "      <td>2022-07-07 13:32:10</td>\n",
       "      <td>2022</td>\n",
       "      <td>Hi everyone, I'm currently looking for jobs te...</td>\n",
       "      <td>hi everyone  i am currently looking for jobs t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text_type       ID        date_created  year  \\\n",
       "124628  submission   ifquow 2020-08-24 19:13:26  2020   \n",
       "125097  submission   gzl2ec 2020-06-09 15:11:27  2020   \n",
       "126727  submission  147gsfl 2023-06-12 10:22:19  2023   \n",
       "126869  submission  12pqx6m 2023-04-17 22:00:27  2023   \n",
       "127216  submission   vtelex 2022-07-07 13:32:10  2022   \n",
       "\n",
       "                                                long_text  \\\n",
       "124628                            GUYS WE MADE IT!!! YAY    \n",
       "125097  PSA: Immigration to Canada, Australia, NZ Hell...   \n",
       "126727  How to reach people who are asking for money/g...   \n",
       "126869  How do you plan to spend your Eid holiday? Any...   \n",
       "127216  Hi everyone, I'm currently looking for jobs te...   \n",
       "\n",
       "                                               clean_text  \n",
       "124628                            guys we made it    yay   \n",
       "125097  psa  immigration to canada  australia  nz hell...  \n",
       "126727  how to reach people who are asking for money g...  \n",
       "126869  how do you plan to spend your eid holiday  any...  \n",
       "127216  hi everyone  i am currently looking for jobs t...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove `\\n` from text\n",
    "data['clean_text'] = data['clean_text'].str.replace('\\n', ' ')\n",
    "#remove `\\t` from text\n",
    "data['clean_text'] = data['clean_text'].str.replace('\\t', ' ')\n",
    "\n",
    "data.iloc[rows_to_check].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6136be-a4c6-42eb-bb70-25956df658b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>g. remove digits</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfad143d-2c3c-42ae-8bd5-7e2e597307fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33736</th>\n",
       "      <td>comment</td>\n",
       "      <td>ik6k7ib</td>\n",
       "      <td>2022-08-14 02:16:45</td>\n",
       "      <td>2022</td>\n",
       "      <td>I think the system is too big. You could insta...</td>\n",
       "      <td>i think the system is too big you could instal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52558</th>\n",
       "      <td>comment</td>\n",
       "      <td>icps7pg</td>\n",
       "      <td>2022-06-17 18:53:45</td>\n",
       "      <td>2022</td>\n",
       "      <td>bro</td>\n",
       "      <td>bro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127352</th>\n",
       "      <td>submission</td>\n",
       "      <td>tjwnth</td>\n",
       "      <td>2022-03-22 10:31:32</td>\n",
       "      <td>2022</td>\n",
       "      <td>Traveller's question - how/where to buy female...</td>\n",
       "      <td>traveller s question how where to buy female l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33414</th>\n",
       "      <td>comment</td>\n",
       "      <td>g1abqxl</td>\n",
       "      <td>2020-08-13 06:32:20</td>\n",
       "      <td>2020</td>\n",
       "      <td>Are looking into buying in bulk ?</td>\n",
       "      <td>are looking into buying in bulk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77424</th>\n",
       "      <td>comment</td>\n",
       "      <td>eug9zto</td>\n",
       "      <td>2019-07-22 17:52:30</td>\n",
       "      <td>2019</td>\n",
       "      <td>That generation of Gulf Arabs spoke Urdu well,...</td>\n",
       "      <td>that generation of gulf arabs spoke urdu well ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text_type       ID        date_created  year  \\\n",
       "33736      comment  ik6k7ib 2022-08-14 02:16:45  2022   \n",
       "52558      comment  icps7pg 2022-06-17 18:53:45  2022   \n",
       "127352  submission   tjwnth 2022-03-22 10:31:32  2022   \n",
       "33414      comment  g1abqxl 2020-08-13 06:32:20  2020   \n",
       "77424      comment  eug9zto 2019-07-22 17:52:30  2019   \n",
       "\n",
       "                                                long_text  \\\n",
       "33736   I think the system is too big. You could insta...   \n",
       "52558                                                 bro   \n",
       "127352  Traveller's question - how/where to buy female...   \n",
       "33414                   Are looking into buying in bulk ?   \n",
       "77424   That generation of Gulf Arabs spoke Urdu well,...   \n",
       "\n",
       "                                               clean_text  \n",
       "33736   i think the system is too big you could instal...  \n",
       "52558                                                 bro  \n",
       "127352  traveller s question how where to buy female l...  \n",
       "33414                     are looking into buying in bulk  \n",
       "77424   that generation of gulf arabs spoke urdu well ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(lambda text:\n",
    "                                              ' '.join (word for word in text.split() \n",
    "                                                        if word.isalpha()))\n",
    "\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f304f-90fe-4662-9b3f-be286f298242",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>h. Remove words with less than three characters</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17417d19-3bcd-4553-afb3-371fdbe560ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6095</th>\n",
       "      <td>comment</td>\n",
       "      <td>el2q29a</td>\n",
       "      <td>2019-04-17 07:22:15</td>\n",
       "      <td>2019</td>\n",
       "      <td>As a previous poster mentioned, angel investor...</td>\n",
       "      <td>previous poster mentioned angel investors inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62668</th>\n",
       "      <td>comment</td>\n",
       "      <td>ga4h28s</td>\n",
       "      <td>2020-10-26 05:21:03</td>\n",
       "      <td>2020</td>\n",
       "      <td>Bruh really thought he could pass on a VW comm...</td>\n",
       "      <td>bruh really thought could pass commercial his own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58150</th>\n",
       "      <td>comment</td>\n",
       "      <td>jop76jh</td>\n",
       "      <td>2023-06-19 17:06:39</td>\n",
       "      <td>2023</td>\n",
       "      <td>Such a good point. \\n \\nSide note.. I’m gratef...</td>\n",
       "      <td>such good point side note grateful have two hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71470</th>\n",
       "      <td>comment</td>\n",
       "      <td>gfwulud</td>\n",
       "      <td>2020-12-15 15:23:48</td>\n",
       "      <td>2020</td>\n",
       "      <td>Start small and work hard! Apply everywhere a...</td>\n",
       "      <td>start small and work hard apply everywhere and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91935</th>\n",
       "      <td>comment</td>\n",
       "      <td>fvn05re</td>\n",
       "      <td>2020-06-22 18:14:42</td>\n",
       "      <td>2020</td>\n",
       "      <td>Filli is the real deal</td>\n",
       "      <td>filli the real deal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_type       ID        date_created  year  \\\n",
       "6095    comment  el2q29a 2019-04-17 07:22:15  2019   \n",
       "62668   comment  ga4h28s 2020-10-26 05:21:03  2020   \n",
       "58150   comment  jop76jh 2023-06-19 17:06:39  2023   \n",
       "71470   comment  gfwulud 2020-12-15 15:23:48  2020   \n",
       "91935   comment  fvn05re 2020-06-22 18:14:42  2020   \n",
       "\n",
       "                                               long_text  \\\n",
       "6095   As a previous poster mentioned, angel investor...   \n",
       "62668  Bruh really thought he could pass on a VW comm...   \n",
       "58150  Such a good point. \\n \\nSide note.. I’m gratef...   \n",
       "71470   Start small and work hard! Apply everywhere a...   \n",
       "91935                             Filli is the real deal   \n",
       "\n",
       "                                              clean_text  \n",
       "6095   previous poster mentioned angel investors inve...  \n",
       "62668  bruh really thought could pass commercial his own  \n",
       "58150  such good point side note grateful have two hu...  \n",
       "71470  start small and work hard apply everywhere and...  \n",
       "91935                                filli the real deal  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(lambda text: ' '.join(word for word in text.split() if len(word) >= 3))\n",
    "\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53912268-14d6-4453-91b4-f59f75099992",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>i. Lemmatization</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80ca5f1b-5d72-4f6d-be14-7f308c360611",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec229df5-feb0-48c6-8fc2-64f0f6df3a14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code block took 76.68 minutes to complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124628</th>\n",
       "      <td>submission</td>\n",
       "      <td>ifquow</td>\n",
       "      <td>2020-08-24 19:13:26</td>\n",
       "      <td>2020</td>\n",
       "      <td>GUYS WE MADE IT!!! YAY</td>\n",
       "      <td>guy make yay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125097</th>\n",
       "      <td>submission</td>\n",
       "      <td>gzl2ec</td>\n",
       "      <td>2020-06-09 15:11:27</td>\n",
       "      <td>2020</td>\n",
       "      <td>PSA: Immigration to Canada, Australia, NZ Hell...</td>\n",
       "      <td>psa immigration canada australia hello dubai r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126727</th>\n",
       "      <td>submission</td>\n",
       "      <td>147gsfl</td>\n",
       "      <td>2023-06-12 10:22:19</td>\n",
       "      <td>2023</td>\n",
       "      <td>How to reach people who are asking for money/g...</td>\n",
       "      <td>how reach people who be ask for money grocery ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126869</th>\n",
       "      <td>submission</td>\n",
       "      <td>12pqx6m</td>\n",
       "      <td>2023-04-17 22:00:27</td>\n",
       "      <td>2023</td>\n",
       "      <td>How do you plan to spend your Eid holiday? Any...</td>\n",
       "      <td>how you plan spend your eid holiday any fun ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127216</th>\n",
       "      <td>submission</td>\n",
       "      <td>vtelex</td>\n",
       "      <td>2022-07-07 13:32:10</td>\n",
       "      <td>2022</td>\n",
       "      <td>Hi everyone, I'm currently looking for jobs te...</td>\n",
       "      <td>everyone currently look for job temporary perm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text_type       ID        date_created  year  \\\n",
       "124628  submission   ifquow 2020-08-24 19:13:26  2020   \n",
       "125097  submission   gzl2ec 2020-06-09 15:11:27  2020   \n",
       "126727  submission  147gsfl 2023-06-12 10:22:19  2023   \n",
       "126869  submission  12pqx6m 2023-04-17 22:00:27  2023   \n",
       "127216  submission   vtelex 2022-07-07 13:32:10  2022   \n",
       "\n",
       "                                                long_text  \\\n",
       "124628                            GUYS WE MADE IT!!! YAY    \n",
       "125097  PSA: Immigration to Canada, Australia, NZ Hell...   \n",
       "126727  How to reach people who are asking for money/g...   \n",
       "126869  How do you plan to spend your Eid holiday? Any...   \n",
       "127216  Hi everyone, I'm currently looking for jobs te...   \n",
       "\n",
       "                                               clean_text  \n",
       "124628                                       guy make yay  \n",
       "125097  psa immigration canada australia hello dubai r...  \n",
       "126727  how reach people who be ask for money grocery ...  \n",
       "126869  how you plan spend your eid holiday any fun ac...  \n",
       "127216  everyone currently look for job temporary perm...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "data['clean_text'] = data['clean_text'].apply(lambda text:\n",
    "                                              ' '.join(token.lemma_ for token in nlp(text)))\n",
    "\n",
    "print (f\"This code block took {(time.time() - start_time)/60 :.2f} minutes to complete\")\n",
    "\n",
    "data.iloc[rows_to_check].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d9447-fb54-4696-9101-f38598646a10",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>j. Remove common stop words</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04888005-8259-4595-b693-f95b6e39bf7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis code block took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes to complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m data\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m text:\n\u001b[0;32m----> 4\u001b[0m                                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m                                                         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word\u001b[38;5;241m.\u001b[39mis_stop]))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis code block took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes to complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m data\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/spacy/language.py:1042\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/spacy/pipeline/tok2vec.py:126\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m--> 126\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/layers/with_array.py:43\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/layers/with_array.py:78\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     76\u001b[0m lengths \u001b[38;5;241m=\u001b[39m NUMPY_OPS\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[1;32m     77\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[0;32m---> 78\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[1;32m     81\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/layers/residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[0;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] \u001b[38;5;241m+\u001b[39m Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 291 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projectenv/lib/python3.10/site-packages/thinc/layers/maxout.py:53\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[0;32m---> 53\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[1;32m     55\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "data['clean_text'] = data['clean_text'].apply(lambda text:\n",
    "                                              ' '.join([word.text for word in nlp(text) \n",
    "                                                        if not word.is_stop]))\n",
    "\n",
    "print (f\"This code block took {(time.time() - start_time)/60 :.2f} minutes to complete\")\n",
    "\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686b780-468d-42fd-be4c-3c30f548b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check top words after removal of common stop words\n",
    "\n",
    "#list of all words in the dataframe\n",
    "all_words = [word for text in data['clean_text'] for word in text.split()]\n",
    "\n",
    "#frequency of word occurrence\n",
    "fdist = FreqDist(all_words)\n",
    "\n",
    "common_words_tuples= fdist.most_common(100)\n",
    "common_words = [word for word, freq in common_words_tuples]\n",
    "\n",
    "#rare_words_dict = fdist.most_common()[-20:-1]\n",
    "#rare_words = [word for word, freq in fdist.items() if freq <= 10]\n",
    "\n",
    "#table of common words\n",
    "#common_words_table = PrettyTable(['word', 'count'])\n",
    "#for word, count in common_words.items():\n",
    "#    common_words_table.add_row([word, count])\n",
    "\n",
    "#print (len(common_words),'\\n\\n',rare_words)\n",
    "print (f'Common words: The top 20 most common words in the dataset are: {common_words}')\n",
    "#print ('\\n')\n",
    "#print (f'Rare words: There are {len(rare_words)} words that occur less than or equal to 10 times in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22a05d-f863-4cc6-baa7-5ac11c7c26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud of most frequent words\n",
    "\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(\n",
    "                width=800, \n",
    "                height=400,  \n",
    "                background_color=\"black\", \n",
    "                colormap=\"Paired\").generate_from_frequencies(#dictionary of word and their frequency of occurrence\n",
    "                                                        FreqDist(\n",
    "                                                            [word for text in data['clean_text'] for word in text.split()])\n",
    "                        )\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b240605-f4db-4c87-8788-46b9ef8195f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create custom stop words list\n",
    "#custom_sw = rare_words + common_words #create list holding common and rare words\n",
    "#custom_sw = set(custom_sw) #remove any duplicates\n",
    "\n",
    "#len(custom_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589d083-7e64-4a11-82e4-2ec65732635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove custom stop words from dataset\n",
    "#data['clean_text'] = data['clean_text'].apply(lambda text: ' '.join([word for word in text.split() if word not in custom_sw]))\n",
    "\n",
    "#data.sample(n=5)                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44c205-1ca7-40ad-9fb0-4ab6d6147456",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>k. remove extra whitespaces</b></h3>\n",
    "</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba4163-e260-4c27-8907-78945d3b7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['clean_text'].str.strip().str.replace(r'\\s+', ' ', regex = True)\n",
    "\n",
    "sample_rows = [5786,18460, 103391]\n",
    "\n",
    "data.loc[sample_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de431c-e0cb-4102-8b28-119d7ff05cda",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>l. word tokenization</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fbcece-0a1f-471d-9763-31a667e22372",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "data['tokens'] = data['clean_text'].apply(lambda text: word_tokenize(text))\n",
    "\n",
    "print (f\"This code block took {(time.time() - start_time)/60 :.2f} minutes to complete\")\n",
    "\n",
    "data.iloc[rows_to_check].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae534471-6575-44e7-9e6f-287ab9d851bb",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>m. insert word count</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa3776-3923-4d5c-bece-0602fa5a33b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_count'] = data['tokens'].apply (lambda tokens_list: len(tokens_list))\n",
    "\n",
    "data.sort_values(by='word_count', ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803db1d1-0e3d-4c71-ab69-c11c603e62ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>n. remove subset for manual labelling</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c1b97-26ce-4f90-82dd-af348d33f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load list of sample_subset indices\n",
    "\n",
    "#sampling done previously by randomly selecting entries from each year\n",
    "\n",
    "with open('../../../Data/sample_subset_index.txt', 'r') as file:\n",
    "    subset_ids = [line.strip() for line in file]\n",
    "\n",
    "subset_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd763db2-6361-4bb7-81ef-80d3d04a31c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create unlabelled sample subset \n",
    "subset_data = data[data['ID'].isin(subset_ids)]\n",
    "subset_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4443c-7426-4fad-895f-de035d779a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get index of entries in sampled subset\n",
    "subset_index = data[data['ID'].isin(subset_ids)].index.to_list()\n",
    "\n",
    "#remove sample subset from data\n",
    "\n",
    "training_data = data.drop(subset_index, axis = 0)\n",
    "\n",
    "\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a0905-24ff-4320-8be0-195ed4f1dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b4bec-b471-4697-a42a-5941f61dac80",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>o. remove short entries</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00b772-25d2-4148-bd13-98732ee896cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only rows with more than 3 word\n",
    "lda_training = training_data[training_data['word_count'] > 3]\n",
    "\n",
    "lda_training.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2b0a4-69d4-48ba-9be3-811700848b58",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h3><b>p. save training dataset for LDAModel</b></h3>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac35ef1-cf0a-49e3-836c-07c943cf1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "filename = '../../../Data/lda_train.csv'\n",
    "\n",
    "def export_csv():\n",
    "    '''\n",
    "    export pre-processed data to CSV\n",
    "    '''\n",
    "    lda_training.to_csv(filename, index_label = 'index', quoting = csv.QUOTE_ALL, header = True)\n",
    "\n",
    "export_csv()\n",
    "\n",
    "print ('file saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664bf0a-b9c5-494f-b929-ae6436aebd9b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213db07-e27e-488a-8ab7-739c7693809b",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h2><b>IV. Feature Extraction</b></h2>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ba423-6eac-4d6a-af88-703e202994e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cleaned data\n",
    "\n",
    "def list_converter(text):\n",
    "    #to revert list->str conversion from pd.read_csv\n",
    "    return ast.literal_eval(text)\n",
    "\n",
    "\n",
    "lda_data = pd.read_csv('../../../Data/lda_train.csv', converters ={'tokens':list_converter})\n",
    "lda_data = lda_data.drop(columns = ['index'])\n",
    "lda_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b02e3-2d91-4159-ace8-98b7e89658a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6d633-f7fa-40c8-915b-e4d68428c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert df['tokens'] to list of strings for bag-of-words model\n",
    "docs = lda_data['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a591cea1-98d7-405f-9b27-a796035e3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef4ed2-b0c0-4d0d-877c-3939b3ad538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of unique words\n",
    "\n",
    "unique_words = set([word for text in docs for word in text])\n",
    "\n",
    "print (f'There are {len(unique_words)} unique words in the dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08cfb3-e5b6-4fbe-8d2d-b418f00ce471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigrams - code from gensim documentation page\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e337064-1876-43ef-8ca4-1eaec3ea08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64100e16-7422-4f4a-85a8-f5812fd79edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from gensim bag of words documentation page\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=50, no_above=0.50)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed2fea-907d-4d76-9ad1-92aa071f1c97",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f105a-c745-467d-9a4d-2cbd165639eb",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-family: Calibri Light;\">\n",
    "  <h2><b>V. TRAIN MODEL</b></h2>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693902a-f4f9-4c98-b5c3-82ccded41fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#retrain, filter extreme to no_abovr = 70, topics = limit to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068c2e3-7661-4d19-8417-e322832fec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(5, 51, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a773f-34b0-4403-8573-c18c7a338aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim documentation at https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#sphx-glr-auto-examples-tutorials-run-lda-py\n",
    "#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/?expand_article=1#9createbigramandtrigrammodels\n",
    "#https://github.com/clevyclev/Deep-Learning-Projects/blob/master/Latent%20Dirichlet%20Allocation%20-%20Bag%20of%20Words%20and%20TF-IDF/Latent_dirichlet_allocation.py\n",
    "\n",
    "#training parameters\n",
    "chunksize = 5000\n",
    "passes = 10\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "id2word = dictionary.id2token\n",
    "temp = dictionary[0] #to \"load\" the dictionary\n",
    "\n",
    "#range of topics\n",
    "topics_range = np.arange (5,51,step = 5)\n",
    "\n",
    "# Lists to hold metrics\n",
    "model_coherence_cv = []\n",
    "#model_coherence_umass = []\n",
    "#model_coherence_cnpmi = []\n",
    "#model_perplexity = []\n",
    "#topic_diversities = []\n",
    "\n",
    "start_time = time.time()\n",
    "for num_topics in topics_range:\n",
    "    \n",
    "    # Train LDA model\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         chunksize=chunksize,\n",
    "                         alpha='auto',\n",
    "                         eta='auto',\n",
    "                         passes=passes,\n",
    "                         iterations=iterations,\n",
    "                         num_topics=num_topics,\n",
    "                         per_word_topics=True,\n",
    "                         random_state=80)\n",
    "    \n",
    "    # Compute c_v score\n",
    "    c_v = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "    cv_lda = c_v.get_coherence()\n",
    "    model_coherence_cv.append(cv_lda)\n",
    "    \n",
    "    # Compute u_mass score\n",
    "    #u_mass = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='u_mass')\n",
    "    #umass_lda = u_mass.get_coherence()\n",
    "    #model_coherence_umass.append(umass_lda)\n",
    "    \n",
    "    # Compute c_npmi score\n",
    "    #c_npmi = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_npmi')\n",
    "    #cnpmi_lda = c_npmi.get_coherence()\n",
    "    #model_coherence_cnpmi.append(cnpmi_lda)\n",
    "    \n",
    "    # Compute perplexity\n",
    "    #perplexity = lda_model.log_perplexity(corpus)\n",
    "    #model_perplexity.append(perplexity)\n",
    "    \n",
    "    # Compute topic diversity\n",
    "    #top_n = 10  # You can adjust this value\n",
    "    #top_words = [word for topic_id in range(num_topics) for word, _ in lda_model.show_topic(topic_id, topn=top_n)]\n",
    "    #diversity = len(set(top_words)) / (num_topics * top_n)\n",
    "    #topic_diversities.append(diversity)\n",
    "\n",
    "print(f\"This model took {(time.time() - start_time)/60 :.2f} minutes to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7ea03-81af-41f7-8b6a-5c4552653951",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(topics_range, model_coherence_cv)\n",
    "plt.xlabel('number of topics')\n",
    "plt.ylabel('coherence score')\n",
    "plt.title('Coherence Score vs. Number of Topics')\n",
    "plt.xticks(topics_range)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa32ff-5e8b-4c33-a270-9bbddff74fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training parameters\n",
    "num_topics = 5\n",
    "chunksize = 5000\n",
    "passes = 10\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "id2word = dictionary.id2token\n",
    "temp = dictionary[0] #to \"load\" the dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#train LDA model\n",
    "lda_model = LdaModel(corpus = corpus,\n",
    "                             id2word = dictionary,\n",
    "                             chunksize = chunksize,\n",
    "                             alpha = 'auto',\n",
    "                             eta = 'auto',\n",
    "                             passes = passes,\n",
    "                             iterations = iterations,\n",
    "                             num_topics = num_topics,\n",
    "                             per_word_topics = True,\n",
    "                             random_state = 180,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6431776-9a25-4404-937c-2624c0fbd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check coherence and perplexity\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0789111-afe0-4a3b-a5db-366a2e2b2b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "#number of topics\n",
    "num_topics = lda_model.num_topics\n",
    "\n",
    "# Number of words to display for each topic\n",
    "num_words = 10\n",
    "\n",
    "#define color map\n",
    "colors = cm.viridis(np.linspace(0,1, num_topics))\n",
    "\n",
    "#subplot layout\n",
    "rows = 3\n",
    "cols = int(np.ceil(num_topics/rows))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "\n",
    "# Iterate through the topics\n",
    "for topic_num in range(num_topics):\n",
    "    # top words and their probabilities for each topic\n",
    "    top_words = lda_model.show_topic(topic_num, topn=num_words)\n",
    "    \n",
    "    # Separate the words and probabilities\n",
    "    topic_words, probs = zip(*top_words)\n",
    "    \n",
    "    #create subplot\n",
    "    plt.subplot(rows, cols, topic_num +1)\n",
    "    \n",
    "    # Plot the words and probabilities as a horizontal bar chart\n",
    "    plt.barh(topic_words, probs, color = colors[topic_num])\n",
    "    plt.xlabel('Probability')\n",
    "    plt.title(f'Topic {topic_num}')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the highest probability at the top\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4059fab4-b07d-41ed-b697-e727ccfe7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary, sort_topics = False)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08586a2-4792-4cd8-b357-38bfcedca187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "projectenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
