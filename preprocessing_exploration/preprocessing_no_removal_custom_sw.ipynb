{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a695a6-2a99-4cb6-93aa-155a55358ce0",
   "metadata": {},
   "source": [
    "# **TEXT PREPROCESSING FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdce37f-9850-4e32-8b6f-851fdcd196ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re #regular expressions library for text manipulation\n",
    "import string\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import ast\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "#NLP libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "import spacymoji\n",
    "import emoji\n",
    "import contractions\n",
    "\n",
    "import itertools\n",
    "from autocorrect import Speller\n",
    "\n",
    "#for wordclouds\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2d063-b4bd-4693-93aa-9e91745d67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional nlp models\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bdd9eb-eb7f-471a-85f7-d9487e20ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments (filepath):\n",
    "    #import data\n",
    "    df = pd.read_csv(filepath, low_memory = False)\n",
    "    \n",
    "    #remove deleted comments if any\n",
    "    df = df[df.Body != '[deleted]']\n",
    "    df = df.dropna(subset=['Body'])\n",
    "    \n",
    "    #remove comments with missing id\n",
    "    drop_index = df[df.isSubmitter.isnull()].index\n",
    "    df.drop(drop_index, inplace = True)\n",
    "    \n",
    "    #remove duplicates if any\n",
    "    df = df.drop_duplicates(subset =['ID'], ignore_index = True)\n",
    "    df.reset_index (drop = True, inplace = True)\n",
    "    \n",
    "    #correct data types and column label\n",
    "    df['Date_Created'] = pd.to_datetime(df['Date_Created'])\n",
    "    df['year'] = df['Date_Created'].dt.year\n",
    "    df['Score'] = df['Score'].astype('int') \n",
    "    df.rename(columns = {'Author_ID': \"Author\"}, inplace = True) \n",
    "    \n",
    "    #rename 'Body' column to text\n",
    "    df.rename(columns = {'Body': 'long_text',\n",
    "                        'Date_Created': 'date_created'}, inplace = True)\n",
    "    \n",
    "    #remove unnecessary columns\n",
    "    df.drop(columns = ['Unnamed: 0', 'Author', 'Score',\n",
    "       'Parent_ID', 'Submission_ID', 'Subreddit', 'isParent', 'isSubmitter'], inplace = True)\n",
    "    \n",
    "    #remove any extra whitespace in column labels\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    #include column to denote row is comment entry\n",
    "    df['text_type'] = 'comment'\n",
    "    \n",
    "    #rearrange column order\n",
    "    df = df[['text_type','ID','date_created', 'year', 'long_text']]\n",
    "    \n",
    "    \n",
    "        \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a26fc-c77d-4fa1-b024-925082583679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_submissions(filepath):\n",
    "    df = pd.read_csv(filepath, low_memory = False)\n",
    "    \n",
    "    #drop duplicate posts\n",
    "    df = df.drop_duplicates(subset =['ID'], ignore_index = True)\n",
    "    df.reset_index (drop = True, inplace = True)\n",
    "    \n",
    "    #create column for post title + post text\n",
    "    df['long_text'] = df['Title']+ \" \" +df['Post Text'].fillna('')\n",
    "    \n",
    "    #adjust data types\n",
    "    df['Date Created'] = pd.to_datetime(df['Date Created'])\n",
    "    df['year'] = df['year'].astype('int')\n",
    "    \n",
    "    #rename columns\n",
    "    df.rename(columns = {'Date Created': 'date_created'}, inplace = True)\n",
    "    \n",
    "    #remove unwanted columns\n",
    "    df = df.drop(columns = ['Unnamed: 0', 'Title','Post Text', 'Score',\n",
    "       'Total Comments', 'Post URL', 'SubReddit','Unnamed: 0.1'])\n",
    "    \n",
    "    #include column to denote row is comment entry\n",
    "    df['text_type'] = 'submission'\n",
    "    \n",
    "    #reorder columns - 'ID', 'Post Text'\n",
    "    df  = df[['text_type','ID', 'date_created','year', 'long_text']]\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ed964-bfc8-42da-84e9-467bf55f0bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#access dataset files\n",
    "folder_path = os.path.join(\"..\", \"Data\")\n",
    "file_type = \"*.csv\"\n",
    "\n",
    "#list of dataset file paths\n",
    "document_path = glob(os.path.join(folder_path, file_type))\n",
    "\n",
    "document_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c5107-b995-4491-b345-76472ef96aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_filepath = '../Data/comments.csv'\n",
    "submissions_filepath = '../Data/full_posts.csv'\n",
    "\n",
    "\n",
    "data = pd.concat([clean_comments(comments_filepath), clean_submissions(submissions_filepath)], ignore_index = True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8ed76-b7f8-4915-b6be-7ed5e7772659",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_group = data.groupby(by='year')\n",
    "for year, group in year_group:\n",
    "    print (year,len(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d117253-4f49-4eff-8f67-76a0082b7bb9",
   "metadata": {},
   "source": [
    "## **TEXT PREPROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa4d85b-f5a6-4fda-b60f-11677f061d61",
   "metadata": {},
   "source": [
    "### **Convert all Text to Lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154efa8-75b1-4736-b133-1cf18e202985",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['long_text'].apply(lambda text: text.lower())\n",
    "\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa3ea4-aefb-4851-9963-25bcb891e25a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Expand Word Contractions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2adc791-a998-4032-b4b9-bdd376f65695",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(lambda text: contractions.fix(text)) \n",
    "\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c151e-1e2a-41b3-870a-a149a963d687",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Remove URLs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e0629-0b47-4d63-acdd-55fe0ef022da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71417843-cf72-4260-8036-8900f14a07c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#index of rows with urls\n",
    "html_index = data[data['long_text'].str.contains(\"https\")].index\n",
    "data.loc[html_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1344f04-2e39-4759-88c6-d32619d04189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex pattern for urls\n",
    "url_pattern = r'https?://\\S+'\n",
    "#replace url with empty string\n",
    "data['clean_text'] = data['clean_text'].apply(lambda text: re.sub(url_pattern, ' ', text, flags=re.MULTILINE))\n",
    "\n",
    "data.loc[html_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916698f5-9fc9-43e3-b64a-c9f5dc9371a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Remove Accents from Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af6084-b233-4214-85a9-74c7cb3ac67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(\n",
    "                                            lambda text: unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8'))\n",
    "\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65038a-27df-4258-8fe9-605d1f4a993f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Remove Punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100ec2e-76b2-41b3-a55e-828e035fcd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index of some rows with punctuations\n",
    "checker_list = ['ifquow','gzl2ec','147gsfl','vtelex',\n",
    " '12pqx6m','fuxrd2','2ui6wu','l4gz0u','14f4uyi','14f8d30']\n",
    "\n",
    "rows_to_check = data[data['ID'].isin(checker_list)].index.tolist()\n",
    "\n",
    "rows_to_check.extend([32003, 116022,18460,5786,30109])\n",
    "\n",
    "rows_to_check.extend(html_index)\n",
    "\n",
    "print(rows_to_check[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1394dc63-71fe-4d2f-8ec0-0b00b1da533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex pattern for punctuations\n",
    "punctuation_pattern = r'[^\\w\\s]'\n",
    "\n",
    "#remove punctuations using `re.sub() method\n",
    "data['clean_text'] = data['clean_text'].apply(lambda text: re.sub(r'[^\\w\\s]', ' ', text))\n",
    "\n",
    "data.iloc[rows_to_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f0e72-ffa9-4581-a1de-f8cae4f7187a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Remove New Line & Tab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ecfff7-a66c-44f2-80bd-0f7fc09264a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove `\\n` from text\n",
    "data['clean_text'] = data['clean_text'].str.replace('\\n', ' ')\n",
    "#remove `\\t` from text\n",
    "data['clean_text'] = data['clean_text'].str.replace('\\t', ' ')\n",
    "\n",
    "data.iloc[rows_to_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6136be-a4c6-42eb-bb70-25956df658b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Remove Digits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad143d-2c3c-42ae-8bd5-7e2e597307fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(lambda text: ' '.join (word for word in text.split() if word.isalpha()))\n",
    "\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53912268-14d6-4453-91b4-f59f75099992",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **LEMMATIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca5f1b-5d72-4f6d-be14-7f308c360611",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec229df5-feb0-48c6-8fc2-64f0f6df3a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data['clean_text'] = data['clean_text'].apply(lambda text: [token.lemma_ for token in nlp(' '.join(text))])\n",
    "#data['clean_text'] = data['clean_text'].apply(lambda text: [token.lemma_ for token in nlp(text)])\n",
    "data['clean_text'] = data['clean_text'].apply(lambda text: ' '.join(token.lemma_ for token in nlp(text)))\n",
    "\n",
    "data.iloc[rows_to_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d9447-fb54-4696-9101-f38598646a10",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Remove Stop Words - SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04888005-8259-4595-b693-f95b6e39bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "data['clean_text'] = data['clean_text'].apply(lambda text: ' '.join([word.text for word in nlp(text) if not word.is_stop]))\n",
    "\n",
    "data.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686b780-468d-42fd-be4c-3c30f548b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check top words after removal of common stop words\n",
    "\n",
    "#list of all words in the dataframe\n",
    "all_words = [word for text in data['clean_text'] for word in text.split()]\n",
    "\n",
    "#frequency of word occurrence\n",
    "fdist = FreqDist(all_words)\n",
    "\n",
    "common_words_tuples= fdist.most_common(100)\n",
    "common_words = [word for word, freq in common_words_tuples]\n",
    "\n",
    "#rare_words_dict = fdist.most_common()[-20:-1]\n",
    "#rare_words = [word for word, freq in fdist.items() if freq <= 10]\n",
    "\n",
    "#table of common words\n",
    "#common_words_table = PrettyTable(['word', 'count'])\n",
    "#for word, count in common_words.items():\n",
    "#    common_words_table.add_row([word, count])\n",
    "\n",
    "#print (len(common_words),'\\n\\n',rare_words)\n",
    "print (f'Common words: The top 20 most common words in the dataset are: {common_words}')\n",
    "#print ('\\n')\n",
    "#print (f'Rare words: There are {len(rare_words)} words that occur less than or equal to 10 times in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22a05d-f863-4cc6-baa7-5ac11c7c26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud of most frequent words\n",
    "\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(\n",
    "                width=800, \n",
    "                height=400,  \n",
    "                background_color=\"black\", \n",
    "                colormap=\"Paired\").generate_from_frequencies(#dictionary of word and their frequency of occurrence\n",
    "                                                        FreqDist(\n",
    "                                                            [word for text in data['clean_text'] for word in text.split()])\n",
    "                        )\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b240605-f4db-4c87-8788-46b9ef8195f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create custom stop words list\n",
    "custom_sw = rare_words + common_words #create list holding common and rare words\n",
    "custom_sw = set(custom_sw) #remove any duplicates\n",
    "\n",
    "len(custom_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589d083-7e64-4a11-82e4-2ec65732635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove custom stop words from dataset\n",
    "data['clean_text'] = data['clean_text'].apply(lambda text: ' '.join([word for word in text.split() if word not in custom_sw]))\n",
    "\n",
    "data.sample(n=5)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ceba5-c609-4cda-9821-41b01e7c451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check top words after removal of common stop words\n",
    "\n",
    "#list of all words in the dataframe\n",
    "all_words = [word for text in data['clean_text'] for word in text.split()]\n",
    "\n",
    "#frequency of word occurrence\n",
    "fdist = FreqDist(all_words)\n",
    "\n",
    "#common_words = fdist.most_common(10)\n",
    "\n",
    "#table of common words\n",
    "common_words_table = PrettyTable(['word', 'count'])\n",
    "for word, count in fdist.most_common(10):\n",
    "    common_words_table.add_row([word, count])\n",
    "\n",
    "print (common_words_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad32f9-8cfc-459a-937e-a451fc4f2e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud of most frequent words\n",
    "\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(\n",
    "                width=800, \n",
    "                height=400,  \n",
    "                background_color=\"black\", \n",
    "                colormap=\"Paired\").generate_from_frequencies(#dictionary of word and their frequency of occurrence\n",
    "                                                        FreqDist(\n",
    "                                                            [word for text in data['clean_text'] for word in text.split()])\n",
    "                        )\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44c205-1ca7-40ad-9fb0-4ab6d6147456",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Remove Extra Whitespaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba4163-e260-4c27-8907-78945d3b7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['clean_text'].str.strip().str.replace('\\s+', ' ', regex = True)\n",
    "\n",
    "sample_rows = [5786,18460, 103391, 129297]\n",
    "\n",
    "#redo word_count\n",
    "#data['word_count'] = data['clean_text'].apply (lambda text: len(text.split()))\n",
    "\n",
    "data.loc[sample_rows]#.sort_values(by='word_count', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de431c-e0cb-4102-8b28-119d7ff05cda",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Word Tokenization - NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fbcece-0a1f-471d-9763-31a667e22372",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['clean_text'].apply(lambda text: word_tokenize(text))\n",
    "\n",
    "data.iloc[rows_to_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae534471-6575-44e7-9e6f-287ab9d851bb",
   "metadata": {},
   "source": [
    "## **Insert Word Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa3776-3923-4d5c-bece-0602fa5a33b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_count'] = data['tokens'].apply (lambda tokens_list: len(tokens_list))\n",
    "\n",
    "data.sort_values(by='word_count', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b4bec-b471-4697-a42a-5941f61dac80",
   "metadata": {},
   "source": [
    "## **Remove Short Text Entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c1b97-26ce-4f90-82dd-af348d33f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load list of sample_subset indices\n",
    "\n",
    "#sampling done previously by randomly selecting entries from each year\n",
    "\n",
    "with open('sample_subset_index.txt', 'r') as file:\n",
    "    subset_index = [line.strip() for line in file]\n",
    "\n",
    "subset_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd763db2-6361-4bb7-81ef-80d3d04a31c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create unlabelled sample subset \n",
    "subset_data = data[data['ID'].isin(subset_index)]\n",
    "subset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00b772-25d2-4148-bd13-98732ee896cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only rows with more than 1 word\n",
    "corpus = data[data['word_count'] > 2]\n",
    "\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803db1d1-0e3d-4c71-ab69-c11c603e62ea",
   "metadata": {},
   "source": [
    "## **Remove Subset for manual labelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4443c-7426-4fad-895f-de035d779a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove randomly sampled subset\n",
    "\n",
    "subset_index = corpus[corpus['ID'].isin(subset_index)].index.to_list()\n",
    "\n",
    "#remove sample subset from corpus\n",
    "\n",
    "training_data = corpus.drop(subset_index, axis = 0)\n",
    "training_data.sort_values(by='word_count', ascending = False)\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac35ef1-cf0a-49e3-836c-07c943cf1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#save corpus \n",
    "filename = '../Data/full_data.csv'\n",
    "\n",
    "def export_csv():\n",
    "    '''\n",
    "    export pre-processed data to CSV\n",
    "    '''\n",
    "    training_data.to_csv(filename, index_label = 'index', quoting = csv.QUOTE_ALL, header = True)\n",
    "\n",
    "export_csv()\n",
    "\n",
    "print ('file saved')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7679547-c8c7-4278-8de8-84fb610017ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Translate Emoticons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb12c83-d21b-43cc-aeca-4380cd430469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"def find_emoji(text):\n",
    "    try:\n",
    "        emoticon_details = next(emoji.analyze(text, join_emoji=True))\n",
    "        emoticon = emoticon_details.chars\n",
    "        #translated_emoji = emoji.demojize(emoticon.chars)\n",
    "    except StopIteration:\n",
    "        emoticon = ''  # Handle the case when no emoji is found\n",
    "    return  emoticon\n",
    "\n",
    "data['emoticons'] = data['long_text'].apply (lambda text: find_emoji(text) )\n",
    "data['translated_emojis'] = data['emoticons'].apply(lambda text: emoji.demojize(text))\n",
    "\n",
    "emoji_index = [3709,33734,129114,100878]\n",
    "\n",
    "data.loc[emoji_index]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15173414-a3dc-46d3-aa22-429f2955b6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "projectenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
