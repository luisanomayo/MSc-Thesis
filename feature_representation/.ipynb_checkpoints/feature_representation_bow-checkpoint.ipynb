{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d05aa73-f694-40ed-b2f7-878d14bb0e33",
   "metadata": {},
   "source": [
    "# **FEATURE EXTRACTION - BAG OF WORDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2421a126-1469-401b-b51b-8d35d483eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import ast\n",
    "import re\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "#data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "#NLP & ML libraries\n",
    "from gensim import corpora\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c6bdc3-33ce-46f4-84c7-3e2d1d9d189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed so that code output is deterministic\n",
    "random.seed(0)  # Set the seed for Python's random module\n",
    "np.random.seed(0)  # Set the seed for NumPy's random module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66a28177-4795-41eb-93e7-2f20f968521d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_type</th>\n",
       "      <th>ID</th>\n",
       "      <th>date_created</th>\n",
       "      <th>year</th>\n",
       "      <th>long_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfou07</td>\n",
       "      <td>2021-04-05 13:13:23</td>\n",
       "      <td>2021</td>\n",
       "      <td>I am single and I have not traveled to any cun...</td>\n",
       "      <td>single travel past</td>\n",
       "      <td>[single, travel, past]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtfrgpe</td>\n",
       "      <td>2021-04-05 13:56:09</td>\n",
       "      <td>2021</td>\n",
       "      <td>What happens when you shop at dragon mart...</td>\n",
       "      <td>shop dragon mart</td>\n",
       "      <td>[shop, dragon, mart]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>gthiiwi</td>\n",
       "      <td>2021-04-05 23:18:56</td>\n",
       "      <td>2021</td>\n",
       "      <td>That’s just absolutely hilarious, is this in t...</td>\n",
       "      <td>hilarious spring souk</td>\n",
       "      <td>[hilarious, spring, souk]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comment</td>\n",
       "      <td>gtgfl4c</td>\n",
       "      <td>2021-04-05 18:21:42</td>\n",
       "      <td>2021</td>\n",
       "      <td>Is reel cinema and roxy part of emaar?</td>\n",
       "      <td>reel cinema roxy emaar</td>\n",
       "      <td>[reel, cinema, roxy, emaar]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>gth5wdv</td>\n",
       "      <td>2021-04-05 21:42:41</td>\n",
       "      <td>2021</td>\n",
       "      <td>An innocent redditor here...can someone pls ex...</td>\n",
       "      <td>innocent pls explain everyday</td>\n",
       "      <td>[innocent, pls, explain, everyday]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text_type       ID         date_created  year  \\\n",
       "0   comment  gtfou07  2021-04-05 13:13:23  2021   \n",
       "1   comment  gtfrgpe  2021-04-05 13:56:09  2021   \n",
       "2   comment  gthiiwi  2021-04-05 23:18:56  2021   \n",
       "3   comment  gtgfl4c  2021-04-05 18:21:42  2021   \n",
       "4   comment  gth5wdv  2021-04-05 21:42:41  2021   \n",
       "\n",
       "                                           long_text  \\\n",
       "0  I am single and I have not traveled to any cun...   \n",
       "1       What happens when you shop at dragon mart...   \n",
       "2  That’s just absolutely hilarious, is this in t...   \n",
       "3             Is reel cinema and roxy part of emaar?   \n",
       "4  An innocent redditor here...can someone pls ex...   \n",
       "\n",
       "                      clean_text                              tokens  \\\n",
       "0             single travel past              [single, travel, past]   \n",
       "1               shop dragon mart                [shop, dragon, mart]   \n",
       "2          hilarious spring souk           [hilarious, spring, souk]   \n",
       "3         reel cinema roxy emaar         [reel, cinema, roxy, emaar]   \n",
       "4  innocent pls explain everyday  [innocent, pls, explain, everyday]   \n",
       "\n",
       "   word_count  \n",
       "0           3  \n",
       "1           3  \n",
       "2           3  \n",
       "3           4  \n",
       "4           4  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import cleaned data\n",
    "\n",
    "def list_converter(text):\n",
    "    #to revert list->str conversion from pd.read_csv\n",
    "    return ast.literal_eval(text)\n",
    "\n",
    "\n",
    "data = pd.read_csv('../Data/training_data.csv', converters ={'tokens':list_converter})\n",
    "data = data.drop(columns = ['index'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07470ff3-9ec0-415a-b720-d24378ad2181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65987 entries, 0 to 65986\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   text_type     65987 non-null  object\n",
      " 1   ID            65987 non-null  object\n",
      " 2   date_created  65987 non-null  object\n",
      " 3   year          65987 non-null  int64 \n",
      " 4   long_text     65987 non-null  object\n",
      " 5   clean_text    65987 non-null  object\n",
      " 6   tokens        65987 non-null  object\n",
      " 7   word_count    65987 non-null  int64 \n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5903cae-6eea-4ece-8653-1814fdaeb153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['single', 'travel', 'past'],\n",
       " ['shop', 'dragon', 'mart'],\n",
       " ['hilarious', 'spring', 'souk'],\n",
       " ['reel', 'cinema', 'roxy', 'emaar'],\n",
       " ['innocent', 'pls', 'explain', 'everyday']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert df['tokens'] to list of strings for bag-of-words model\n",
    "docs = data['tokens'].tolist()\n",
    "\n",
    "docs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22d6f846-59c6-40ff-8c23-e6433a1b8178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7577 unique words in the dataset\n"
     ]
    }
   ],
   "source": [
    "#check number of unique words\n",
    "\n",
    "unique_words = set([word for text in docs for word in text])\n",
    "\n",
    "print (f'There are {len(unique_words)} unique words in the dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61f81992-6b19-4644-9608-39e829d5e748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 11:59:01,962 : INFO : collecting all words and their counts\n",
      "2023-08-10 11:59:01,971 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2023-08-10 11:59:02,203 : INFO : PROGRESS: at sentence #10000, processed 104233 words and 89822 word types\n",
      "2023-08-10 11:59:02,389 : INFO : PROGRESS: at sentence #20000, processed 209460 words and 167853 word types\n",
      "2023-08-10 11:59:02,574 : INFO : PROGRESS: at sentence #30000, processed 310795 words and 238385 word types\n",
      "2023-08-10 11:59:02,736 : INFO : PROGRESS: at sentence #40000, processed 416666 words and 310741 word types\n",
      "2023-08-10 11:59:02,928 : INFO : PROGRESS: at sentence #50000, processed 522246 words and 380235 word types\n",
      "2023-08-10 11:59:03,082 : INFO : PROGRESS: at sentence #60000, processed 624316 words and 444929 word types\n",
      "2023-08-10 11:59:03,230 : INFO : collected 503659 token types (unigram + bigrams) from a corpus of 718695 words and 65987 sentences\n",
      "2023-08-10 11:59:03,231 : INFO : merged Phrases<503659 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n",
      "2023-08-10 11:59:03,232 : INFO : Phrases lifecycle event {'msg': 'built Phrases<503659 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000> in 1.27s', 'datetime': '2023-08-10T11:59:03.232125', 'gensim': '4.3.1', 'python': '3.10.1 (v3.10.1:2cd268a3a9, Dec  6 2021, 14:28:59) [Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97617eaa-e894-471e-b6ef-a0d80e92f20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labor', 'law', 'common', 'blatant', 'sale', 'promotional', 'hire', 'filter', 'rife', 'real', 'estate', 'industry', 'young', 'motivation', 'law', 'align', 'progressive', 'practice', 'nature', 'labor_law', 'real_estate', 'labor_law', 'real_estate', 'labor_law', 'real_estate']\n"
     ]
    }
   ],
   "source": [
    "print(docs[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0663fc85-a00f-443f-b7f2-64bf2b2db81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 14:09:48,881 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2023-08-10 14:09:49,206 : INFO : adding document #10000 to Dictionary<13948 unique tokens: ['past', 'single', 'travel', 'dragon', 'dragon_mart']...>\n",
      "2023-08-10 14:09:49,734 : INFO : adding document #20000 to Dictionary<19384 unique tokens: ['past', 'single', 'travel', 'dragon', 'dragon_mart']...>\n",
      "2023-08-10 14:09:50,046 : INFO : adding document #30000 to Dictionary<23135 unique tokens: ['past', 'single', 'travel', 'dragon', 'dragon_mart']...>\n",
      "2023-08-10 14:09:50,337 : INFO : adding document #40000 to Dictionary<26268 unique tokens: ['past', 'single', 'travel', 'dragon', 'dragon_mart']...>\n",
      "2023-08-10 14:09:50,634 : INFO : adding document #50000 to Dictionary<28669 unique tokens: ['past', 'single', 'travel', 'dragon', 'dragon_mart']...>\n",
      "2023-08-10 14:09:50,937 : INFO : adding document #60000 to Dictionary<30505 unique tokens: ['past', 'single', 'travel', 'dragon', 'dragon_mart']...>\n",
      "2023-08-10 14:09:51,216 : INFO : built Dictionary<31889 unique tokens: ['past', 'single', 'travel', 'dragon', 'dragon_mart']...> from 65987 documents (total 835585 corpus positions)\n",
      "2023-08-10 14:09:51,218 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<31889 unique tokens: ['past', 'single', 'travel', 'dragon', 'dragon_mart']...> from 65987 documents (total 835585 corpus positions)\", 'datetime': '2023-08-10T14:09:51.218486', 'gensim': '4.3.1', 'python': '3.10.1 (v3.10.1:2cd268a3a9, Dec  6 2021, 14:28:59) [Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2023-08-10 14:09:51,305 : INFO : discarding 26755 tokens: [('spring_souk', 3), ('reel_cinema', 5), ('roxy', 14), ('amusing', 12), ('mildly', 14), ('misspell', 12), ('decor', 10), ('swear_swear', 3), ('men', 8), ('men_bathroom', 2)]...\n",
      "2023-08-10 14:09:51,310 : INFO : keeping 5134 tokens which were in no less than 20 and no more than 32993 (=50.0%) documents\n",
      "2023-08-10 14:09:51,363 : INFO : resulting dictionary: Dictionary<5134 unique tokens: ['past', 'single', 'travel', 'dragon', 'dragon_mart']...>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 5134\n",
      "Number of documents: 65987\n"
     ]
    }
   ],
   "source": [
    "#from gensim bag of words documentation page\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f0986-77b8-4d7f-95a5-1741cf01810a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "projectenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
